{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching txt-file for book with id 55\n",
      "Downloading book: The Wonderful Wizard of Oz by L. Frank  Baum\n",
      "From source: https://www.gutenberg.org/55/55-0.txt\n",
      "Book 55 loaded.\n",
      "717\n",
      "788\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "import nltk\n",
    "import fnmatch\n",
    "\n",
    "root = '/Users/danie/'\n",
    "base_url = 'https://www.gutenberg.org'\n",
    "\n",
    "#%%\n",
    "gutenberg_response_html = requests.get(base_url + '/browse/scores/top')\n",
    "gutenberg_parsed = bs4.BeautifulSoup(gutenberg_response_html.content, 'html.parser')\n",
    "location = gutenberg_parsed.find(id='books-last30')\n",
    "book_listing_tag = location.next_sibling.next_sibling\n",
    "\n",
    "#%%\n",
    "def get_bookdata(book_listing, k):\n",
    "    data = []\n",
    "    for a_tag in book_listing.find_all('a')[:k]:\n",
    "        book_name = re.match(r'(.*)(\\(\\d+\\))', a_tag.text)\n",
    "        book_name = book_name.group(1).strip()\n",
    "    \n",
    "        book_id = re.match(r'/ebooks/(\\d+)', a_tag.get('href'))\n",
    "        book_id = book_id.group(1)\n",
    "    \n",
    "        book_url = base_url + '/' + book_id + '/'\n",
    "        \n",
    "        data.append({\"id\":book_id, \"name\":book_name, \"url\":book_url, \"fname\":''})\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_bookdata_byname(book_listing, name):\n",
    "    data = []\n",
    "    for a_tag in book_listing.find_all('a'):\n",
    "        book_name = re.match(r'(.*)(\\(\\d+\\))', a_tag.text)\n",
    "        book_name = book_name.group(1).strip()\n",
    "        \n",
    "        if name not in book_name: continue\n",
    "        \n",
    "        book_id = re.match(r'/ebooks/(\\d+)', a_tag.get('href'))\n",
    "        book_id = book_id.group(1)\n",
    "        \n",
    "        book_url = base_url + '/' + book_id + '/'\n",
    "        \n",
    "        data.append({\"id\":book_id, \"name\":book_name, \"url\":book_url, \"fname\":''})\n",
    "        break\n",
    "    \n",
    "    return data\n",
    "        \n",
    "def complete_urls(bookdata):\n",
    "    for i in range(len(bookdata)):\n",
    "        print(\"Searching txt-file for book with id {}\".format(bookdata[i][\"id\"]))\n",
    "        indexurl = bookdata[i][\"url\"]\n",
    "        \n",
    "        bookindex_html = requests.get(indexurl)\n",
    "        bookindex_parsed =  bs4.BeautifulSoup(bookindex_html.content,'html.parser')\n",
    "        bookindex_links = bookindex_parsed.find_all('a')\n",
    "        bookindex_hrefs = [bil['href'] for bil in bookindex_links]\n",
    "        \n",
    "        book_filenames = [ bih for bih in bookindex_hrefs if fnmatch.fnmatch(bih, '*.txt') ] #.*.txt\n",
    "        \n",
    "        bookdata[i][\"url\"] += book_filenames[0]\n",
    "        bookdata[i][\"fname\"] = book_filenames[0]\n",
    "        \n",
    "    return\n",
    "\n",
    "def download_books(bookdata):\n",
    "    for data in bookdata:\n",
    "        print(\"Downloading book: {}\".format(data[\"name\"]))\n",
    "        print(\"From source: {}\".format(data[\"url\"]))\n",
    "        response = requests.get(data[\"url\"], allow_redirects=True)\n",
    "        #txtfiles already made\n",
    "        with open(root + 'txtfiles/' + data[\"fname\"], 'wb') as f:\n",
    "            f.write(response.content)\n",
    "            \n",
    "#%%\n",
    "book_name = 'The Wonderful Wizard of Oz'\n",
    "bookdata = get_bookdata_byname(book_listing_tag, book_name)\n",
    "complete_urls(bookdata)\n",
    "download_books(bookdata)\n",
    "\n",
    "#%%\n",
    "# Download data from local filesystem\n",
    "\n",
    "def load_data_local(bookdata, fileloc):\n",
    "    my_text = []\n",
    "    for data in bookdata:\n",
    "        try:\n",
    "            with open(fileloc + data[\"fname\"], 'r') as f:\n",
    "                my_text.append(f.read())\n",
    "        except:\n",
    "            with open(fileloc + data[\"fname\"], 'r', encoding='ISO-8859-1') as f:\n",
    "                my_text.append(f.read()) \n",
    "        print(\"Book {} loaded.\".format(data['id']))\n",
    "    \n",
    "    return my_text\n",
    "    \n",
    "\n",
    "book_texts = load_data_local(bookdata, root + 'txtfiles/')\n",
    "\n",
    "#%%\n",
    "# Remove the start and end information, so only story text is left\n",
    "\n",
    "def remove_headers(book_texts):\n",
    "    start_header = '*** START'\n",
    "    end_header = '*** END'\n",
    "    new_texts = []\n",
    "    for text in book_texts:\n",
    "        start_loc = text.find(start_header)\n",
    "        print(start_loc)\n",
    "        start_loc = text[start_loc:].find('\\n') + start_loc\n",
    "        print(start_loc)\n",
    "        end_loc = text.find(end_header)\n",
    "        text = text[start_loc : end_loc]\n",
    "        new_texts.append(text)\n",
    "        \n",
    "    return new_texts\n",
    "        \n",
    "book_texts = remove_headers(book_texts)\n",
    "my_text = book_texts[0]\n",
    "\n",
    "#%% Get the paragraphs\n",
    "# b)\n",
    "mytext_paragraphs = paragraphs=re.split('\\n[ \\n]*\\n', my_text)\n",
    "#paragraphs = paragraphs[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1141"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mytext_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_nltk_texts = []\n",
    "for k in range(len(mytext_paragraphs)):\n",
    "    temp_tokenizedtext = nltk.word_tokenize(mytext_paragraphs[k])\n",
    "    temp_nltktext = nltk.Text(temp_tokenizedtext)\n",
    "    paragraph_nltk_texts.append(temp_nltktext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: ...>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_nltk_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_lowercase_texts = []\n",
    "for k in range(len(paragraph_nltk_texts)):\n",
    "    temp_lowercase_text = []\n",
    "    for l in range(len(paragraph_nltk_texts[k])):\n",
    "        lowercase_word = paragraph_nltk_texts[k][l].lower()\n",
    "        temp_lowercase_text.append(lowercase_word)\n",
    "    temp_lowercasetest = nltk.Text(temp_lowercase_text)\n",
    "    paragraph_lowercase_texts.append(temp_lowercase_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS\n",
    "def tagtowordnet(postag):\n",
    "    wordnettag=-1\n",
    "    if postag[0]=='N':\n",
    "        wordnettag='n'\n",
    "    elif postag[0]=='V':\n",
    "        wordnettag='v'\n",
    "    elif postag[0]=='J':\n",
    "        wordnettag='a'\n",
    "    elif postag[0]=='R':\n",
    "        wordnettag='r'\n",
    "    return(wordnettag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Download wordnet resource if you do not have it already\n",
    "nltk.download('wordnet')\n",
    "# Download tagger resource if you do not have it already\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer=nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizetext(nltktexttolemmatize):\n",
    "    # Tag the text with POS tags\n",
    "    taggedtext=nltk.pos_tag(nltktexttolemmatize)\n",
    "    # Lemmatize each word text\n",
    "    lemmatizedtext=[]\n",
    "    for l in range(len(taggedtext)):\n",
    "        # Lemmatize a word using the WordNet converted POS tag\n",
    "        wordtolemmatize=taggedtext[l][0]\n",
    "        wordnettag=tagtowordnet(taggedtext[l][1])\n",
    "        if wordnettag!=-1:\n",
    "            lemmatizedword=lemmatizer.lemmatize(wordtolemmatize,wordnettag)\n",
    "        else:\n",
    "            lemmatizedword=wordtolemmatize\n",
    "        # Store the lemmatized word\n",
    "        lemmatizedtext.append(lemmatizedword)\n",
    "    return(lemmatizedtext)\n",
    "\n",
    "\n",
    "# lemmatization of text\n",
    "#lemmatized_texts = lemmatizetext(lowercase_texts)\n",
    "#lemmatized_texts = nltk.Text(lemmatized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_lemmatizedtexts = []\n",
    "for k in range(len(paragraph_lowercase_texts)):\n",
    "    lemmatizedtext = lemmatizetext(paragraph_lowercase_texts[k])\n",
    "    lemmatizedtext = nltk.Text(lemmatizedtext)\n",
    "    paragraph_lemmatizedtexts.append(lemmatizedtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: have this think in mind , the story...>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_lemmatizedtexts[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "myvocabularies=[]\n",
    "myindices_in_vocabularies=[]\n",
    "# Find the vocabulary of each document\n",
    "for k in range(len(paragraph_lemmatizedtexts)):\n",
    "    # Get unique words and where they occur\n",
    "    temptext=paragraph_lemmatizedtexts[k]\n",
    "    uniqueresults=np.unique(temptext,return_inverse=True)\n",
    "    uniquewords=uniqueresults[0]\n",
    "    wordindices=uniqueresults[1]\n",
    "    # Store the vocabulary and indices of document words in it\n",
    "    myvocabularies.append(uniquewords)\n",
    "    myindices_in_vocabularies.append(wordindices)\n",
    "myvocabularies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempvocabulary=[]\n",
    "for k in range(len(paragraph_lemmatizedtexts)):\n",
    "    tempvocabulary.extend(myvocabularies[k])\n",
    "    \n",
    "# Find the unique elements among all vocabularies\n",
    "uniqueresults=np.unique(tempvocabulary,return_inverse=True)\n",
    "unifiedvocabulary=uniqueresults[0]\n",
    "wordindices=uniqueresults[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate previous indices to unified vocabulary.\n",
    "\n",
    "vocabularystart=0\n",
    "myindices_in_unifiedvocabulary=[]\n",
    "for k in range(len(paragraph_lemmatizedtexts)):\n",
    "    # In order to shift word indices, we must temporarily\n",
    "    # change their data type to a Numpy array\n",
    "    tempindices=np.array(myindices_in_vocabularies[k])\n",
    "    tempindices=tempindices+vocabularystart\n",
    "    tempindices=wordindices[tempindices]\n",
    "    myindices_in_unifiedvocabulary.append(tempindices)\n",
    "    vocabularystart=vocabularystart+len(myvocabularies[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unifiedvocabulary_totaloccurrencecounts=np.zeros((len(unifiedvocabulary),1))\n",
    "unifiedvocabulary_documentcounts=np.zeros((len(unifiedvocabulary),1))\n",
    "unifiedvocabulary_meancounts=np.zeros((len(unifiedvocabulary),1))\n",
    "unifiedvocabulary_countvariances=np.zeros((len(unifiedvocabulary),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n"
     ]
    }
   ],
   "source": [
    "for k in range(len(paragraph_lemmatizedtexts)):\n",
    "    print(k)\n",
    "    occurrencecounts=np.zeros((len(unifiedvocabulary),1))\n",
    "    for l in range(len(myindices_in_unifiedvocabulary[k])):\n",
    "        occurrencecounts[myindices_in_unifiedvocabulary[k][l]]= \\\n",
    "            occurrencecounts[myindices_in_unifiedvocabulary[k][l]]+1\n",
    "    unifiedvocabulary_totaloccurrencecounts= \\\n",
    "        unifiedvocabulary_totaloccurrencecounts+occurrencecounts\n",
    "    unifiedvocabulary_documentcounts= \\\n",
    "        unifiedvocabulary_documentcounts+(occurrencecounts>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean occurrence counts over documents\n",
    "unifiedvocabulary_meancounts= \\\n",
    "    unifiedvocabulary_totaloccurrencecounts/len(paragraph_lemmatizedtexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the' ',' 'and' '.' 'be' 'to' 'of' 'a' 'â\\x80\\x9d' 'in' 'have' 'i' 'he'\n",
      " 'you' 'her' 'she' 'they' 'it' 'that' 'say' 'dorothy' 'as' 'so' 'for'\n",
      " 'with' 'not' 'at' 'but' 'all' 'them' 'do' 'scarecrow' 'his' ';' '?' 'me'\n",
      " 'him' 'my' 'woodman' 'lion' 'come' 'on' 'will' 'oz' 'great' 'when' 'go'\n",
      " 'make' 'â\\x80\\x9ci' 'tin' 'ask' 'little' 'witch' 'this' 'from' 'one'\n",
      " 'could' 'then' 'see' 'there' 'would' 'we' 'if' 'get' 'up' 'out' 'who'\n",
      " 'head' 'can' 'green' 'their' 'back' 'look' '!' 'no' 'think' 'girl' 'down'\n",
      " 'know' 'by' 'toto' 'over' 'answer' 'upon' 'shall' 'find' 'give' 'again'\n",
      " 'good' 'into' 'very' 'now' 'must' 'city' 'wicked' 'where' 'walk' 'after'\n",
      " 'emerald' 'long']\n",
      "[2982. 2703. 1600. 1597. 1433. 1110.  840.  801.  698.  481.  476.  452.\n",
      "  439.  439.  405.  392.  390.  383.  361.  356.  347.  329.  296.  291.\n",
      "  274.  272.  251.  243.  239.  233.  231.  219.  216.  196.  194.  186.\n",
      "  179.  178.  175.  175.  169.  157.  156.  151.  150.  148.  148.  144.\n",
      "  143.  139.  139.  139.  137.  129.  122.  122.  120.  119.  119.  114.\n",
      "  113.  108.  106.  105.  105.  103.  103.  101.  101.  100.  100.   99.\n",
      "   97.   97.   94.   94.   94.   93.   93.   90.   90.   87.   86.   85.\n",
      "   81.   81.   81.   80.   80.   79.   77.   76.   75.   75.   72.   72.\n",
      "   71.   70.   69.   69.]\n"
     ]
    }
   ],
   "source": [
    "#%% Inspect frequent words\n",
    "# Sort words by largest total (or mean) occurrence count\n",
    "highest_totaloccurrences_indices=np.argsort(\\\n",
    "    -1*unifiedvocabulary_totaloccurrencecounts,axis=0)    \n",
    "print(np.squeeze(unifiedvocabulary[\\\n",
    "    highest_totaloccurrences_indices[0:100]]))\n",
    "print(np.squeeze(\\\n",
    "    unifiedvocabulary_totaloccurrencecounts[\\\n",
    "    highest_totaloccurrences_indices[0:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "#%% Vocabulary pruning\n",
    "nltkstopwords=nltk.corpus.stopwords.words('english')\n",
    "pruningdecisions=np.zeros((len(unifiedvocabulary),1))\n",
    "for k in range(len(unifiedvocabulary)):\n",
    "    # Rule 1: check the nltk stop word list\n",
    "    if (unifiedvocabulary[k] in nltkstopwords):\n",
    "        pruningdecisions[k]=1\n",
    "    # Rule 2: if the word is in the top 1% of frequent words\n",
    "    #if (k in highest_totaloccurrences_indices[\\\n",
    "    #    0:int(np.floor(len(unifiedvocabulary)*0.01))]):\n",
    "    #    pruningdecisions[k]=1\n",
    "   # Rule 3: if the word occurs less than 4 times\n",
    "    if(unifiedvocabulary_totaloccurrencecounts[k] < 4):\n",
    "        pruningdecisions[k] = 1\n",
    "    # Rule 4: if the word is too short\n",
    "    if len(unifiedvocabulary[k])<2:\n",
    "        pruningdecisions[k]=1\n",
    "    # Rule 5: if the word is too long\n",
    "    if len(unifiedvocabulary[k])>20:\n",
    "        pruningdecisions[k]=1\n",
    "    # Rule 6: if the word has unwanted characters\n",
    "    # (here for simplicity only a-z allowed)\n",
    "    if unifiedvocabulary[k].isalpha()==False:\n",
    "        pruningdecisions[k]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Get indices of documents to remaining words\n",
    "oldtopruned=[]\n",
    "tempind=-1\n",
    "for k in range(len(unifiedvocabulary)):\n",
    "    if pruningdecisions[k]==0:\n",
    "        tempind=tempind+1\n",
    "        oldtopruned.append(tempind)\n",
    "    else:\n",
    "        oldtopruned.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n"
     ]
    }
   ],
   "source": [
    "#%% Create pruned texts\n",
    "paragraph_prunedtexts=[]\n",
    "myindices_in_prunedvocabulary=[]\n",
    "for k in range(len(paragraph_lemmatizedtexts)):\n",
    "    print(k)\n",
    "    temp_newindices=[]\n",
    "    temp_newdoc=[]\n",
    "    for l in range(len(paragraph_lemmatizedtexts[k])):\n",
    "        temp_oldindex=myindices_in_unifiedvocabulary[k][l]\n",
    "        temp_newindex=oldtopruned[temp_oldindex]\n",
    "        if temp_newindex!=-1:\n",
    "            temp_newindices.append(temp_newindex)\n",
    "            temp_newdoc.append(unifiedvocabulary[temp_oldindex])\n",
    "    paragraph_prunedtexts.append(temp_newdoc)\n",
    "    myindices_in_prunedvocabulary.append(temp_newindices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 word-list after pruning unified vocabulary:\n",
      "\n",
      "['say' 'dorothy' 'scarecrow' 'woodman' 'lion' 'come' 'oz' 'great' 'go'\n",
      " 'make' 'tin' 'ask' 'little' 'witch' 'one' 'could' 'see' 'would' 'get'\n",
      " 'head' 'green' 'back' 'look' 'think' 'girl' 'know' 'toto' 'answer' 'upon'\n",
      " 'find' 'shall' 'give' 'good' 'must' 'city' 'wicked' 'walk' 'emerald'\n",
      " 'long' 'man' 'country' 'room' 'tree' 'away' 'heart' 'like' 'take' 'big'\n",
      " 'time' 'way' 'carry' 'tell' 'saw' 'people' 'us' 'never' 'eye' 'reply'\n",
      " 'stand' 'monkey' 'brain' 'live' 'well' 'many' 'chapter' 'day' 'forest'\n",
      " 'first' 'run' 'road' 'ever' 'friend' 'soon' 'help' 'house' 'around'\n",
      " 'much' 'keep' 'wish' 'wizard' 'arm' 'cry' 'beast' 'sit' 'thing' 'old'\n",
      " 'mouse' 'call' 'land' 'beautiful' 'shoe' 'leave' 'air' 'woman' 'seem'\n",
      " 'put' 'fly' 'quite' 'voice' 'begin']\n",
      "[356. 347. 219. 175. 175. 169. 151. 150. 148. 144. 139. 139. 139. 137.\n",
      " 122. 120. 119. 113. 105. 101. 100.  99.  97.  94.  94.  93.  90.  86.\n",
      "  85.  81.  81.  81.  80.  75.  75.  72.  71.  69.  69.  68.  67.  66.\n",
      "  65.  65.  65.  65.  64.  63.  62.  60.  59.  58.  58.  57.  56.  55.\n",
      "  55.  55.  52.  52.  51.  49.  49.  49.  48.  47.  47.  47.  46.  46.\n",
      "  45.  45.  44.  43.  42.  42.  42.  42.  41.  41.  41.  41.  41.  40.\n",
      "  40.  39.  38.  38.  38.  38.  37.  36.  36.  35.  35.  35.  35.  35.\n",
      "  34.  34.]\n"
     ]
    }
   ],
   "source": [
    "print('Top 100 word-list after pruning unified vocabulary:\\n')\n",
    "remaining_indices = np.squeeze(np.where(pruningdecisions==0)[0])\n",
    "remaining_vocabulary = unifiedvocabulary[remaining_indices]\n",
    "remainingvocabulary_totaloccurrencecounts = unifiedvocabulary_totaloccurrencecounts[remaining_indices]\n",
    "remaining_highest_totaloccurrences_indices = np.argsort(-1*remainingvocabulary_totaloccurrencecounts, axis=0)\n",
    "print(np.squeeze(remaining_vocabulary[remaining_highest_totaloccurrences_indices[0:100]]))\n",
    "print(np.squeeze(remainingvocabulary_totaloccurrencecounts[remaining_highest_totaloccurrences_indices[0:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "C:\\Users\\danie\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "C:\\Users\\danie\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "#%% Create TF-IDF vectors\n",
    "n_docs=len(paragraph_prunedtexts)\n",
    "n_vocab=len(remaining_vocabulary)\n",
    "# Matrix of term frequencies\n",
    "tfmatrix=scipy.sparse.lil_matrix((n_docs,n_vocab))\n",
    "# Row vector of document frequencies\n",
    "dfvector=scipy.sparse.lil_matrix((1,n_vocab))\n",
    "# Loop over documents\n",
    "for k in range(n_docs):\n",
    "    # Row vector of which words occurred in this document\n",
    "    temp_dfvector=scipy.sparse.lil_matrix((1,n_vocab))\n",
    "    # Loop over words\n",
    "    for l in range(len(paragraph_prunedtexts[k])):\n",
    "        # Add current word to term-frequency count and document-count\n",
    "        currentword=myindices_in_prunedvocabulary[k][l]\n",
    "        tfmatrix[k,currentword]=tfmatrix[k,currentword]+1\n",
    "        temp_dfvector[0,currentword]=1\n",
    "    # Add which words occurred in this document to overall document counts\n",
    "    dfvector=dfvector+temp_dfvector\n",
    "\n",
    "# TF:length-normalized frequency\n",
    "for i in range(n_docs):\n",
    "    for j in range(len(tfmatrix.data[i])):\n",
    "        tfmatrix.data[i][j] = tfmatrix.data[i][j]/len(tfmatrix.data[i])\n",
    "        \n",
    "\n",
    "# smoothed logarithmic idf\n",
    "idfvector = np.squeeze(np.array(dfvector.todense()))\n",
    "idfvector = np.log(1 + ((idfvector+1)**-1)*n_docs)        \n",
    "\n",
    "tfidfmatrix = scipy.sparse.lil_matrix((n_docs, n_vocab))\n",
    "for k in range(n_docs):\n",
    "    # tf and idf terms\n",
    "    tfidfmatrix[k,:]=tfmatrix[k,:]*idfvector\n",
    "    \n",
    "# tf-idf matrix\n",
    "#tfidfmatrix = scipy.sparse.lil_matrix((n_docs, n_vocab))\n",
    "for k in range(n_docs):\n",
    "    # find nonzero term frequencies\n",
    "    tempindices = np.nonzero(tfmatrix[k, :])[1]\n",
    "    tfterm = np.squeeze(np.array(tfmatrix[k, tempindices].todense()))\n",
    "    tfidfmatrix[k, tempindices] = tfterm * idfvector[tempindices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Use the TF-IDF matrix as data to be clustered\n",
    "X=tfidfmatrix\n",
    "# Normalize the documents to unit vector norm\n",
    "tempnorms=np.squeeze(np.array(np.sum(X.multiply(X),axis=1)))\n",
    "# If any documents have zero norm, avoid dividing them by zero\n",
    "tempnorms[tempnorms==0]=1\n",
    "X=scipy.sparse.diags(tempnorms**-0.5).dot(X)\n",
    "n_data=np.shape(X)[0]\n",
    "n_dimensions=np.shape(X)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize the Gaussian mixture model, create component parameters\n",
    "def initialize_mixturemodel(X,n_components):\n",
    "    # Create lists of sparse matrices to hold the parameters\n",
    "    n_dimensions=np.shape(X)[1]\n",
    "    n_data = np.shape(X)[0]\n",
    "    mixturemodel_means=scipy.sparse.lil_matrix((n_components,n_dimensions))\n",
    "    mixturemodel_weights=np.zeros((n_components))\n",
    "    mixturemodel_covariances=[]\n",
    "    mixturemodel_inversecovariances=[]\n",
    "    for k in range(n_components):\n",
    "        tempcovariance=scipy.sparse.lil_matrix((n_dimensions,n_dimensions))\n",
    "        mixturemodel_covariances.append(tempcovariance)\n",
    "        tempinvcovariance=scipy.sparse.lil_matrix((n_dimensions,n_dimensions))\n",
    "        mixturemodel_inversecovariances.append(tempinvcovariance)\n",
    "    # Initialize the parameters\n",
    "    for k in range(n_components):\n",
    "        mixturemodel_weights[k]=1/n_components\n",
    "        # Pick a random data point as the initial mean\n",
    "        tempindex=scipy.stats.randint.rvs(low=0,high=n_data)\n",
    "        mixturemodel_means[k]=X[tempindex,:].toarray()\n",
    "        # Initialize the covariance matrix to be spherical\n",
    "        for l in range(n_dimensions):\n",
    "            mixturemodel_covariances[k][l,l]=1\n",
    "            mixturemodel_inversecovariances[k][l,l]=1\n",
    "    return(mixturemodel_weights,mixturemodel_means,mixturemodel_covariances,mixturemodel_inversecovariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_estep(X,mixturemodel_means,mixturemodel_covariances,mixturemodel_inversecovariances,mixturemodel_weights):\n",
    "    # For each component, compute terms that do not involve data\n",
    "    meanterms=np.zeros((n_components))\n",
    "    logdeterminants=np.zeros((n_components))\n",
    "    logconstantterms=np.zeros((n_components))\n",
    "    \n",
    "    for k in range(n_components):\n",
    "        # Compute mu_k*inv(Sigma_k)*mu_k\n",
    "        meanterms[k]=(mixturemodel_means[k,:]*mixturemodel_inversecovariances[k]*mixturemodel_means[k,:].T)[0,0]\n",
    "        # Compute determinant of Sigma_k. For a diagonal matrix\n",
    "        # this is just the product of the main diagonal\n",
    "        logdeterminants[k]=np.sum(np.log(mixturemodel_covariances[k].diagonal(0)))\n",
    "        # Compute constant term beta_k * 1/(|Sigma_k|^1/2)\n",
    "        # Omit the (2pi)^d/2 as it cancels out\n",
    "        logconstantterms[k]=np.log(mixturemodel_weights[k]) - 0.5*logdeterminants[k]\n",
    "    \n",
    "    print('E-step part2 ')\n",
    "    # Compute terms that involve distances of data from components\n",
    "    xnorms=np.zeros((n_data,n_components))\n",
    "    xtimesmu=np.zeros((n_data,n_components))\n",
    "    \n",
    "    for k in range(n_components):\n",
    "        #print(k)\n",
    "        xnorms[:,k]=(X*mixturemodel_inversecovariances[k]*X.T).diagonal(0)\n",
    "        xtimesmu[:,k]=np.squeeze((X*mixturemodel_inversecovariances[k]* mixturemodel_means[k,:].T).toarray())\n",
    "        \n",
    "    xdists=xnorms+np.matlib.repmat(meanterms,n_data,1)-2*xtimesmu\n",
    "    # Substract maximal term before exponent (cancels out) to maintain computational precision\n",
    "    numeratorterms=logconstantterms-xdists/2\n",
    "    numeratorterms-=np.matlib.repmat(np.max(numeratorterms,axis=1),n_components,1).T\n",
    "    numeratorterms=np.exp(numeratorterms)\n",
    "    mixturemodel_componentmemberships=numeratorterms/np.matlib.repmat(np.sum(numeratorterms,axis=1),n_components,1).T\n",
    "    return(mixturemodel_componentmemberships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mstep_sumweights(mixturemodel_componentmemberships):\n",
    "    # Compute total weight per component\n",
    "    mixturemodel_weights=np.sum(mixturemodel_componentmemberships,axis=0)\n",
    "    return(mixturemodel_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mstep_means(X,mixturemodel_componentmemberships,mixturemodel_weights):\n",
    "    # Update component means\n",
    "    mixturemodel_means=scipy.sparse.lil_matrix((n_components,n_dimensions))\n",
    "    for k in range(n_components):\n",
    "        mixturemodel_means[k,:]=\\\n",
    "            np.sum(scipy.sparse.diags(mixturemodel_componentmemberships[:,k]).dot(X),axis=0)\n",
    "        mixturemodel_means[k,:]/=mixturemodel_weights[k]\n",
    "    return(mixturemodel_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mstep_covariances(X,mixturemodel_componentmemberships,mixturemodel_weights,mixturemodel_means):\n",
    "    # Update diagonal component covariance matrices\n",
    "    n_dimensions=np.shape(X)[1]\n",
    "    n_components=np.shape(mixturemodel_componentmemberships)[1]\n",
    "    tempcovariances=np.zeros((n_components,n_dimensions))\n",
    "    mixturemodel_covariances=[]\n",
    "    mixturemodel_inversecovariances=[]\n",
    "    \n",
    "    for k in range(n_components):\n",
    "        tempcovariances[k,:]= np.sum(scipy.sparse.diags(\n",
    "            mixturemodel_componentmemberships[:,k]).dot(\n",
    "            X.multiply(X)),axis=0)-mixturemodel_means[k,:].multiply(mixturemodel_means[k,:])*mixturemodel_weights[k]\n",
    "        tempcovariances[k,:]/=mixturemodel_weights[k]\n",
    "        # Convert to sparse matrices\n",
    "        tempepsilon=1e-10\n",
    "        # Add a small regularization term\n",
    "        temp_covariance=scipy.sparse.diags(tempcovariances[k,:]+tempepsilon)\n",
    "        temp_inversecovariance=scipy.sparse.diags((tempcovariances[k,:]+tempepsilon)**-1)\n",
    "        mixturemodel_covariances.append(temp_covariance)\n",
    "        mixturemodel_inversecovariances.append(temp_inversecovariance)\n",
    "    return(mixturemodel_covariances,mixturemodel_inversecovariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mstep_normalizeweights(mixturemodel_weights):\n",
    "    # Update mixture-component prior probabilities\n",
    "    mixturemodel_weights/=sum(mixturemodel_weights)\n",
    "    return(mixturemodel_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "C:\\Users\\danie\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-step 0\n",
      "E-step part2 \n",
      "M-step 0\n",
      "M-step part1 0\n",
      "M-step part2 0\n",
      "M-step part3 0\n",
      "M-step part4 0\n",
      "E-step 1\n",
      "E-step part2 \n",
      "M-step 1\n",
      "M-step part1 1\n",
      "M-step part2 1\n",
      "M-step part3 1\n",
      "M-step part4 1\n",
      "E-step 2\n",
      "E-step part2 \n",
      "M-step 2\n",
      "M-step part1 2\n",
      "M-step part2 2\n",
      "M-step part3 2\n",
      "M-step part4 2\n",
      "E-step 3\n",
      "E-step part2 \n",
      "M-step 3\n",
      "M-step part1 3\n",
      "M-step part2 3\n",
      "M-step part3 3\n",
      "M-step part4 3\n",
      "E-step 4\n",
      "E-step part2 \n",
      "M-step 4\n",
      "M-step part1 4\n",
      "M-step part2 4\n",
      "M-step part3 4\n",
      "M-step part4 4\n",
      "E-step 5\n",
      "E-step part2 \n",
      "M-step 5\n",
      "M-step part1 5\n",
      "M-step part2 5\n",
      "M-step part3 5\n",
      "M-step part4 5\n",
      "E-step 6\n",
      "E-step part2 \n",
      "M-step 6\n",
      "M-step part1 6\n",
      "M-step part2 6\n",
      "M-step part3 6\n",
      "M-step part4 6\n",
      "E-step 7\n",
      "E-step part2 \n",
      "M-step 7\n",
      "M-step part1 7\n",
      "M-step part2 7\n",
      "M-step part3 7\n",
      "M-step part4 7\n",
      "E-step 8\n",
      "E-step part2 \n",
      "M-step 8\n",
      "M-step part1 8\n",
      "M-step part2 8\n",
      "M-step part3 8\n",
      "M-step part4 8\n",
      "E-step 9\n",
      "E-step part2 \n",
      "M-step 9\n",
      "M-step part1 9\n",
      "M-step part2 9\n",
      "M-step part3 9\n",
      "M-step part4 9\n",
      "E-step 10\n",
      "E-step part2 \n",
      "M-step 10\n",
      "M-step part1 10\n",
      "M-step part2 10\n",
      "M-step part3 10\n",
      "M-step part4 10\n",
      "E-step 11\n",
      "E-step part2 \n",
      "M-step 11\n",
      "M-step part1 11\n",
      "M-step part2 11\n",
      "M-step part3 11\n",
      "M-step part4 11\n",
      "E-step 12\n",
      "E-step part2 \n",
      "M-step 12\n",
      "M-step part1 12\n",
      "M-step part2 12\n",
      "M-step part3 12\n",
      "M-step part4 12\n",
      "E-step 13\n",
      "E-step part2 \n",
      "M-step 13\n",
      "M-step part1 13\n",
      "M-step part2 13\n",
      "M-step part3 13\n",
      "M-step part4 13\n",
      "E-step 14\n",
      "E-step part2 \n",
      "M-step 14\n",
      "M-step part1 14\n",
      "M-step part2 14\n",
      "M-step part3 14\n",
      "M-step part4 14\n",
      "E-step 15\n",
      "E-step part2 \n",
      "M-step 15\n",
      "M-step part1 15\n",
      "M-step part2 15\n",
      "M-step part3 15\n",
      "M-step part4 15\n",
      "E-step 16\n",
      "E-step part2 \n",
      "M-step 16\n",
      "M-step part1 16\n",
      "M-step part2 16\n",
      "M-step part3 16\n",
      "M-step part4 16\n",
      "E-step 17\n",
      "E-step part2 \n",
      "M-step 17\n",
      "M-step part1 17\n",
      "M-step part2 17\n",
      "M-step part3 17\n",
      "M-step part4 17\n",
      "E-step 18\n",
      "E-step part2 \n",
      "M-step 18\n",
      "M-step part1 18\n",
      "M-step part2 18\n",
      "M-step part3 18\n",
      "M-step part4 18\n",
      "E-step 19\n",
      "E-step part2 \n",
      "M-step 19\n",
      "M-step part1 19\n",
      "M-step part2 19\n",
      "M-step part3 19\n",
      "M-step part4 19\n",
      "E-step 20\n",
      "E-step part2 \n",
      "M-step 20\n",
      "M-step part1 20\n",
      "M-step part2 20\n",
      "M-step part3 20\n",
      "M-step part4 20\n",
      "E-step 21\n",
      "E-step part2 \n",
      "M-step 21\n",
      "M-step part1 21\n",
      "M-step part2 21\n",
      "M-step part3 21\n",
      "M-step part4 21\n",
      "E-step 22\n",
      "E-step part2 \n",
      "M-step 22\n",
      "M-step part1 22\n",
      "M-step part2 22\n",
      "M-step part3 22\n",
      "M-step part4 22\n",
      "E-step 23\n",
      "E-step part2 \n",
      "M-step 23\n",
      "M-step part1 23\n",
      "M-step part2 23\n",
      "M-step part3 23\n",
      "M-step part4 23\n",
      "E-step 24\n",
      "E-step part2 \n",
      "M-step 24\n",
      "M-step part1 24\n",
      "M-step part2 24\n",
      "M-step part3 24\n",
      "M-step part4 24\n",
      "E-step 25\n",
      "E-step part2 \n",
      "M-step 25\n",
      "M-step part1 25\n",
      "M-step part2 25\n",
      "M-step part3 25\n",
      "M-step part4 25\n",
      "E-step 26\n",
      "E-step part2 \n",
      "M-step 26\n",
      "M-step part1 26\n",
      "M-step part2 26\n",
      "M-step part3 26\n",
      "M-step part4 26\n",
      "E-step 27\n",
      "E-step part2 \n",
      "M-step 27\n",
      "M-step part1 27\n",
      "M-step part2 27\n",
      "M-step part3 27\n",
      "M-step part4 27\n",
      "E-step 28\n",
      "E-step part2 \n",
      "M-step 28\n",
      "M-step part1 28\n",
      "M-step part2 28\n",
      "M-step part3 28\n",
      "M-step part4 28\n",
      "E-step 29\n",
      "E-step part2 \n",
      "M-step 29\n",
      "M-step part1 29\n",
      "M-step part2 29\n",
      "M-step part3 29\n",
      "M-step part4 29\n",
      "E-step 30\n",
      "E-step part2 \n",
      "M-step 30\n",
      "M-step part1 30\n",
      "M-step part2 30\n",
      "M-step part3 30\n",
      "M-step part4 30\n",
      "E-step 31\n",
      "E-step part2 \n",
      "M-step 31\n",
      "M-step part1 31\n",
      "M-step part2 31\n",
      "M-step part3 31\n",
      "M-step part4 31\n",
      "E-step 32\n",
      "E-step part2 \n",
      "M-step 32\n",
      "M-step part1 32\n",
      "M-step part2 32\n",
      "M-step part3 32\n",
      "M-step part4 32\n",
      "E-step 33\n",
      "E-step part2 \n",
      "M-step 33\n",
      "M-step part1 33\n",
      "M-step part2 33\n",
      "M-step part3 33\n",
      "M-step part4 33\n",
      "E-step 34\n",
      "E-step part2 \n",
      "M-step 34\n",
      "M-step part1 34\n",
      "M-step part2 34\n",
      "M-step part3 34\n",
      "M-step part4 34\n",
      "E-step 35\n",
      "E-step part2 \n",
      "M-step 35\n",
      "M-step part1 35\n",
      "M-step part2 35\n",
      "M-step part3 35\n",
      "M-step part4 35\n",
      "E-step 36\n",
      "E-step part2 \n",
      "M-step 36\n",
      "M-step part1 36\n",
      "M-step part2 36\n",
      "M-step part3 36\n",
      "M-step part4 36\n",
      "E-step 37\n",
      "E-step part2 \n",
      "M-step 37\n",
      "M-step part1 37\n",
      "M-step part2 37\n",
      "M-step part3 37\n",
      "M-step part4 37\n",
      "E-step 38\n",
      "E-step part2 \n",
      "M-step 38\n",
      "M-step part1 38\n",
      "M-step part2 38\n",
      "M-step part3 38\n",
      "M-step part4 38\n",
      "E-step 39\n",
      "E-step part2 \n",
      "M-step 39\n",
      "M-step part1 39\n",
      "M-step part2 39\n",
      "M-step part3 39\n",
      "M-step part4 39\n",
      "E-step 40\n",
      "E-step part2 \n",
      "M-step 40\n",
      "M-step part1 40\n",
      "M-step part2 40\n",
      "M-step part3 40\n",
      "M-step part4 40\n",
      "E-step 41\n",
      "E-step part2 \n",
      "M-step 41\n",
      "M-step part1 41\n",
      "M-step part2 41\n",
      "M-step part3 41\n",
      "M-step part4 41\n",
      "E-step 42\n",
      "E-step part2 \n",
      "M-step 42\n",
      "M-step part1 42\n",
      "M-step part2 42\n",
      "M-step part3 42\n",
      "M-step part4 42\n",
      "E-step 43\n",
      "E-step part2 \n",
      "M-step 43\n",
      "M-step part1 43\n",
      "M-step part2 43\n",
      "M-step part3 43\n",
      "M-step part4 43\n",
      "E-step 44\n",
      "E-step part2 \n",
      "M-step 44\n",
      "M-step part1 44\n",
      "M-step part2 44\n",
      "M-step part3 44\n",
      "M-step part4 44\n",
      "E-step 45\n",
      "E-step part2 \n",
      "M-step 45\n",
      "M-step part1 45\n",
      "M-step part2 45\n",
      "M-step part3 45\n",
      "M-step part4 45\n",
      "E-step 46\n",
      "E-step part2 \n",
      "M-step 46\n",
      "M-step part1 46\n",
      "M-step part2 46\n",
      "M-step part3 46\n",
      "M-step part4 46\n",
      "E-step 47\n",
      "E-step part2 \n",
      "M-step 47\n",
      "M-step part1 47\n",
      "M-step part2 47\n",
      "M-step part3 47\n",
      "M-step part4 47\n",
      "E-step 48\n",
      "E-step part2 \n",
      "M-step 48\n",
      "M-step part1 48\n",
      "M-step part2 48\n",
      "M-step part3 48\n",
      "M-step part4 48\n",
      "E-step 49\n",
      "E-step part2 \n",
      "M-step 49\n",
      "M-step part1 49\n",
      "M-step part2 49\n",
      "M-step part3 49\n",
      "M-step part4 49\n",
      "E-step 50\n",
      "E-step part2 \n",
      "M-step 50\n",
      "M-step part1 50\n",
      "M-step part2 50\n",
      "M-step part3 50\n",
      "M-step part4 50\n",
      "E-step 51\n",
      "E-step part2 \n",
      "M-step 51\n",
      "M-step part1 51\n",
      "M-step part2 51\n",
      "M-step part3 51\n",
      "M-step part4 51\n",
      "E-step 52\n",
      "E-step part2 \n",
      "M-step 52\n",
      "M-step part1 52\n",
      "M-step part2 52\n",
      "M-step part3 52\n",
      "M-step part4 52\n",
      "E-step 53\n",
      "E-step part2 \n",
      "M-step 53\n",
      "M-step part1 53\n",
      "M-step part2 53\n",
      "M-step part3 53\n",
      "M-step part4 53\n",
      "E-step 54\n",
      "E-step part2 \n",
      "M-step 54\n",
      "M-step part1 54\n",
      "M-step part2 54\n",
      "M-step part3 54\n",
      "M-step part4 54\n",
      "E-step 55\n",
      "E-step part2 \n",
      "M-step 55\n",
      "M-step part1 55\n",
      "M-step part2 55\n",
      "M-step part3 55\n",
      "M-step part4 55\n",
      "E-step 56\n",
      "E-step part2 \n",
      "M-step 56\n",
      "M-step part1 56\n",
      "M-step part2 56\n",
      "M-step part3 56\n",
      "M-step part4 56\n",
      "E-step 57\n",
      "E-step part2 \n",
      "M-step 57\n",
      "M-step part1 57\n",
      "M-step part2 57\n",
      "M-step part3 57\n",
      "M-step part4 57\n",
      "E-step 58\n",
      "E-step part2 \n",
      "M-step 58\n",
      "M-step part1 58\n",
      "M-step part2 58\n",
      "M-step part3 58\n",
      "M-step part4 58\n",
      "E-step 59\n",
      "E-step part2 \n",
      "M-step 59\n",
      "M-step part1 59\n",
      "M-step part2 59\n",
      "M-step part3 59\n",
      "M-step part4 59\n",
      "E-step 60\n",
      "E-step part2 \n",
      "M-step 60\n",
      "M-step part1 60\n",
      "M-step part2 60\n",
      "M-step part3 60\n",
      "M-step part4 60\n",
      "E-step 61\n",
      "E-step part2 \n",
      "M-step 61\n",
      "M-step part1 61\n",
      "M-step part2 61\n",
      "M-step part3 61\n",
      "M-step part4 61\n",
      "E-step 62\n",
      "E-step part2 \n",
      "M-step 62\n",
      "M-step part1 62\n",
      "M-step part2 62\n",
      "M-step part3 62\n",
      "M-step part4 62\n",
      "E-step 63\n",
      "E-step part2 \n",
      "M-step 63\n",
      "M-step part1 63\n",
      "M-step part2 63\n",
      "M-step part3 63\n",
      "M-step part4 63\n",
      "E-step 64\n",
      "E-step part2 \n",
      "M-step 64\n",
      "M-step part1 64\n",
      "M-step part2 64\n",
      "M-step part3 64\n",
      "M-step part4 64\n",
      "E-step 65\n",
      "E-step part2 \n",
      "M-step 65\n",
      "M-step part1 65\n",
      "M-step part2 65\n",
      "M-step part3 65\n",
      "M-step part4 65\n",
      "E-step 66\n",
      "E-step part2 \n",
      "M-step 66\n",
      "M-step part1 66\n",
      "M-step part2 66\n",
      "M-step part3 66\n",
      "M-step part4 66\n",
      "E-step 67\n",
      "E-step part2 \n",
      "M-step 67\n",
      "M-step part1 67\n",
      "M-step part2 67\n",
      "M-step part3 67\n",
      "M-step part4 67\n",
      "E-step 68\n",
      "E-step part2 \n",
      "M-step 68\n",
      "M-step part1 68\n",
      "M-step part2 68\n",
      "M-step part3 68\n",
      "M-step part4 68\n",
      "E-step 69\n",
      "E-step part2 \n",
      "M-step 69\n",
      "M-step part1 69\n",
      "M-step part2 69\n",
      "M-step part3 69\n",
      "M-step part4 69\n",
      "E-step 70\n",
      "E-step part2 \n",
      "M-step 70\n",
      "M-step part1 70\n",
      "M-step part2 70\n",
      "M-step part3 70\n",
      "M-step part4 70\n",
      "E-step 71\n",
      "E-step part2 \n",
      "M-step 71\n",
      "M-step part1 71\n",
      "M-step part2 71\n",
      "M-step part3 71\n",
      "M-step part4 71\n",
      "E-step 72\n",
      "E-step part2 \n",
      "M-step 72\n",
      "M-step part1 72\n",
      "M-step part2 72\n",
      "M-step part3 72\n",
      "M-step part4 72\n",
      "E-step 73\n",
      "E-step part2 \n",
      "M-step 73\n",
      "M-step part1 73\n",
      "M-step part2 73\n",
      "M-step part3 73\n",
      "M-step part4 73\n",
      "E-step 74\n",
      "E-step part2 \n",
      "M-step 74\n",
      "M-step part1 74\n",
      "M-step part2 74\n",
      "M-step part3 74\n",
      "M-step part4 74\n",
      "E-step 75\n",
      "E-step part2 \n",
      "M-step 75\n",
      "M-step part1 75\n",
      "M-step part2 75\n",
      "M-step part3 75\n",
      "M-step part4 75\n",
      "E-step 76\n",
      "E-step part2 \n",
      "M-step 76\n",
      "M-step part1 76\n",
      "M-step part2 76\n",
      "M-step part3 76\n",
      "M-step part4 76\n",
      "E-step 77\n",
      "E-step part2 \n",
      "M-step 77\n",
      "M-step part1 77\n",
      "M-step part2 77\n",
      "M-step part3 77\n",
      "M-step part4 77\n",
      "E-step 78\n",
      "E-step part2 \n",
      "M-step 78\n",
      "M-step part1 78\n",
      "M-step part2 78\n",
      "M-step part3 78\n",
      "M-step part4 78\n",
      "E-step 79\n",
      "E-step part2 \n",
      "M-step 79\n",
      "M-step part1 79\n",
      "M-step part2 79\n",
      "M-step part3 79\n",
      "M-step part4 79\n",
      "E-step 80\n",
      "E-step part2 \n",
      "M-step 80\n",
      "M-step part1 80\n",
      "M-step part2 80\n",
      "M-step part3 80\n",
      "M-step part4 80\n",
      "E-step 81\n",
      "E-step part2 \n",
      "M-step 81\n",
      "M-step part1 81\n",
      "M-step part2 81\n",
      "M-step part3 81\n",
      "M-step part4 81\n",
      "E-step 82\n",
      "E-step part2 \n",
      "M-step 82\n",
      "M-step part1 82\n",
      "M-step part2 82\n",
      "M-step part3 82\n",
      "M-step part4 82\n",
      "E-step 83\n",
      "E-step part2 \n",
      "M-step 83\n",
      "M-step part1 83\n",
      "M-step part2 83\n",
      "M-step part3 83\n",
      "M-step part4 83\n",
      "E-step 84\n",
      "E-step part2 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M-step 84\n",
      "M-step part1 84\n",
      "M-step part2 84\n",
      "M-step part3 84\n",
      "M-step part4 84\n",
      "E-step 85\n",
      "E-step part2 \n",
      "M-step 85\n",
      "M-step part1 85\n",
      "M-step part2 85\n",
      "M-step part3 85\n",
      "M-step part4 85\n",
      "E-step 86\n",
      "E-step part2 \n",
      "M-step 86\n",
      "M-step part1 86\n",
      "M-step part2 86\n",
      "M-step part3 86\n",
      "M-step part4 86\n",
      "E-step 87\n",
      "E-step part2 \n",
      "M-step 87\n",
      "M-step part1 87\n",
      "M-step part2 87\n",
      "M-step part3 87\n",
      "M-step part4 87\n",
      "E-step 88\n",
      "E-step part2 \n",
      "M-step 88\n",
      "M-step part1 88\n",
      "M-step part2 88\n",
      "M-step part3 88\n",
      "M-step part4 88\n",
      "E-step 89\n",
      "E-step part2 \n",
      "M-step 89\n",
      "M-step part1 89\n",
      "M-step part2 89\n",
      "M-step part3 89\n",
      "M-step part4 89\n",
      "E-step 90\n",
      "E-step part2 \n",
      "M-step 90\n",
      "M-step part1 90\n",
      "M-step part2 90\n",
      "M-step part3 90\n",
      "M-step part4 90\n",
      "E-step 91\n",
      "E-step part2 \n",
      "M-step 91\n",
      "M-step part1 91\n",
      "M-step part2 91\n",
      "M-step part3 91\n",
      "M-step part4 91\n",
      "E-step 92\n",
      "E-step part2 \n",
      "M-step 92\n",
      "M-step part1 92\n",
      "M-step part2 92\n",
      "M-step part3 92\n",
      "M-step part4 92\n",
      "E-step 93\n",
      "E-step part2 \n",
      "M-step 93\n",
      "M-step part1 93\n",
      "M-step part2 93\n",
      "M-step part3 93\n",
      "M-step part4 93\n",
      "E-step 94\n",
      "E-step part2 \n",
      "M-step 94\n",
      "M-step part1 94\n",
      "M-step part2 94\n",
      "M-step part3 94\n",
      "M-step part4 94\n",
      "E-step 95\n",
      "E-step part2 \n",
      "M-step 95\n",
      "M-step part1 95\n",
      "M-step part2 95\n",
      "M-step part3 95\n",
      "M-step part4 95\n",
      "E-step 96\n",
      "E-step part2 \n",
      "M-step 96\n",
      "M-step part1 96\n",
      "M-step part2 96\n",
      "M-step part3 96\n",
      "M-step part4 96\n",
      "E-step 97\n",
      "E-step part2 \n",
      "M-step 97\n",
      "M-step part1 97\n",
      "M-step part2 97\n",
      "M-step part3 97\n",
      "M-step part4 97\n",
      "E-step 98\n",
      "E-step part2 \n",
      "M-step 98\n",
      "M-step part1 98\n",
      "M-step part2 98\n",
      "M-step part3 98\n",
      "M-step part4 98\n",
      "E-step 99\n",
      "E-step part2 \n",
      "M-step 99\n",
      "M-step part1 99\n",
      "M-step part2 99\n",
      "M-step part3 99\n",
      "M-step part4 99\n"
     ]
    }
   ],
   "source": [
    "#%% Perform the EM algorithm iterations\n",
    "def perform_emalgorithm(X,n_components,n_emiterations):\n",
    "    mixturemodel_weights,mixturemodel_means,mixturemodel_covariances,\\\n",
    "    mixturemodel_inversecovariances=initialize_mixturemodel(X,n_components)\n",
    "    for t in range(n_emiterations):\n",
    "        # ====== E-step: Compute the component membership\n",
    "        # probabilities of each data point ======\n",
    "        print('E-step ' + str(t))\n",
    "        mixturemodel_componentmemberships=run_estep(X,mixturemodel_means,mixturemodel_covariances,\\\n",
    "        mixturemodel_inversecovariances,mixturemodel_weights)\n",
    "        # ====== M-step: update component parameters======\n",
    "        print('M-step ' + str(t))\n",
    "        print('M-step part1 ' + str(t))\n",
    "        mixturemodel_weights=run_mstep_sumweights(mixturemodel_componentmemberships)\n",
    "        print('M-step part2 ' + str(t))\n",
    "        mixturemodel_means=run_mstep_means(X,mixturemodel_componentmemberships,mixturemodel_weights)\n",
    "        print('M-step part3 ' + str(t))\n",
    "        mixturemodel_covariances,mixturemodel_inversecovariances=run_mstep_covariances(X,\\\n",
    "        mixturemodel_componentmemberships,mixturemodel_weights,mixturemodel_means)\n",
    "        print('M-step part4 ' + str(t))\n",
    "        mixturemodel_weights=run_mstep_normalizeweights(mixturemodel_weights)\n",
    "    return(mixturemodel_weights,mixturemodel_means,mixturemodel_covariances,\\\n",
    "mixturemodel_inversecovariances)\n",
    "# Try out the functions we just defined on the data\n",
    "n_components=10\n",
    "n_emiterations=100\n",
    "mixturemodel_weights,mixturemodel_means,mixturemodel_covariances,\\\n",
    "mixturemodel_inversecovariances = perform_emalgorithm(X,n_components,n_emiterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "look supper lonely suppose sure surely listen surprise surprised\n",
      "1\n",
      "lucky magic tinsmith man manage mane low many mark\n",
      "2\n",
      "pat pave perch person pick piece pin pass place\n",
      "3\n",
      "pole fairy prairie presently pull everywhere purple everyone push\n",
      "4\n",
      "inside inquire sweep immediately imagine swiftly table hour hot\n",
      "5\n",
      "others ought palace part party pat patch order pave\n",
      "6\n",
      "hung humbug however story hour hot horse hope home\n",
      "7\n",
      "oblige offer often oil old open order ought paint\n",
      "8\n",
      "marry search ruby careful ought name joyfully chapter sadly\n",
      "9\n",
      "fairy nearly whirl explain expect exclaim need move exactly\n"
     ]
    }
   ],
   "source": [
    "for k in range(n_components):\n",
    "    print(k)\n",
    "    highest_dimensionweight_indices=np.argsort(-np.squeeze(mixturemodel_means[k,:].toarray()),axis=0)\n",
    "\n",
    "    print(' '.join(remaining_vocabulary[highest_dimensionweight_indices[1:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[   0  876  635  633  631  420   11 1140    1    7]\n",
      "\n",
      "1\n",
      "[   0  876  635  633  631  420   11 1140    7    3]\n",
      "\n",
      "2\n",
      "[   0  876  635  633  631  420   11 1140    3    1]\n",
      "\n",
      "3\n",
      "[   0  876  635  633  631  420   11 1140    7    3]\n",
      "\n",
      "4\n",
      "[   0  876  635  633  631  420   11 1140    7    1]\n",
      "\n",
      "5\n",
      "[   0  876  635  633  631  420   11 1140    7    3]\n",
      "\n",
      "6\n",
      "[   0  876  635  633  631  420   11 1140    3    7]\n",
      "\n",
      "7\n",
      "[   0  876  635  633  631  420   11 1140    7    1]\n",
      "\n",
      "8\n",
      "[   0  876  635  633  631  420   11 1140    1    7]\n",
      "\n",
      "9\n",
      "[   0  876  635  633  631  420   11 1140    3    7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Version 2 - Get documents closest to component mean, i.e. highest p(d|k).\n",
    "# ---The computation of distances here is the same as done in the E-step of EM---\n",
    "# For each component, compute terms that do not involve data\n",
    "meanterms=np.zeros((n_components))\n",
    "logdeterminants=np.zeros((n_components))\n",
    "logconstantterms=np.zeros((n_components))\n",
    "for k in range(n_components):\n",
    "    # Compute mu_k*inv(Sigma_k)*mu_k\n",
    "    meanterms[k]=(mixturemodel_means[k,:]*mixturemodel_inversecovariances[k]*mixturemodel_means[k,:].T)[0,0]\n",
    "\n",
    "# Compute terms that involve distances of data from components\n",
    "xnorms=np.zeros((n_data,n_components))\n",
    "xtimesmu=np.zeros((n_data,n_components))\n",
    "\n",
    "for k in range(n_components):\n",
    "    xnorms[:,k]=(X*mixturemodel_inversecovariances[k]*X.T).diagonal(0)\n",
    "    xtimesmu[:,k]=np.squeeze((X*mixturemodel_inversecovariances[k]*mixturemodel_means[k,:].T).toarray())\n",
    "\n",
    "xdists=xnorms+np.matlib.repmat(meanterms,n_data,1)-2*xtimesmu\n",
    "\n",
    "for k in range(n_components):\n",
    "    tempdists=np.array(np.squeeze(xdists[:,k]))\n",
    "    highest_componentprob_indices=np.argsort(-1*tempdists,axis=0)\n",
    "    print(k)\n",
    "    print(highest_componentprob_indices[0:10])\n",
    "    print(' '.join(paragraph_nltk_texts[highest_componentprob_indices[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EX 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find out the amout of words in paragraphs\n",
    "word_count = np.zeros((len(paragraph_lemmatizedtexts), 1))\n",
    "for k in range(len(paragraph_lemmatizedtexts)):\n",
    "    counting = len(paragraph_lemmatizedtexts[k])\n",
    "    word_count[k] = counting\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([234.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_longest_paragraph = np.argsort(-1*word_count, axis=0)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([505], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_longest_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she leave dorothy alone and go back to the others . these she also lead to room , and each one of them find himself lodge in a very pleasant part of the palace . of course this politeness be waste on the scarecrow ; for when he find himself alone in his room he stand stupidly in one spot , just within the doorway , to wait till morning . it would not rest him to lie down , and he could not close his eye ; so he remain all night star at a little spider which be weave its web in a corner of the room , just as if it be not one of the most wonderful room in the world . the tin woodman lay down on his bed from force of habit , for he remember when he be make of flesh ; but not be able to sleep , he pass the night move his joint up and down to make sure they keep in good work order . the lion would have prefer a bed of dried leaf in the forest , and do not like be shut up in a room ; but he have too much sense to let this worry him , so he spring upon the bed and roll himself up like a cat and purr himself asleep in a minute .\n"
     ]
    }
   ],
   "source": [
    "#The longest paragraph \n",
    "print(' '.join(paragraph_lemmatizedtexts[505]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she leave dorothy alone and go back to the others . these she also lead to room , and each one of them find himself lodge in a very pleasant part of the palace . of course this politeness be waste on the scarecrow ; for when he find himself alone in his room he stand stupidly in one spot , just within the doorway , to wait till morning . it would not rest him to lie down , and he could not close his eye ; so he remain all night star at a little spider which be weave its web in a corner of the room , just as if it be not one of the most wonderful room in the world . the tin woodman lay down on his bed from force of habit , for he remember when he be make of flesh ; but not be able to sleep , he pass the night move his joint up and down to make sure they keep in good work order . the lion would have prefer a bed of dried leaf in the forest , and do not like be shut up in a room ; but he have too much sense to let this worry him , so he spring upon the bed and roll himself up like a cat and purr himself asleep in a minute .'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longestpara=(' '.join(paragraph_lemmatizedtexts[505]))\n",
    "longestpara\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['s'], dtype='<U1')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longestmyvocabularies=[]\n",
    "longestmyindices_in_vocabularies=[]\n",
    "# Find the vocabulary of each document\n",
    "for k in range(len(longestpara)):\n",
    "    # Get unique words and where they occur\n",
    "    temptext=longestpara[k]\n",
    "    uniqueresults=np.unique(temptext,return_inverse=True)\n",
    "    uniquewords=uniqueresults[0]\n",
    "    wordindices=uniqueresults[1]\n",
    "    # Store the vocabulary and indices of document words in it\n",
    "    longestmyvocabularies.append(uniquewords)\n",
    "    longestmyindices_in_vocabularies.append(wordindices)\n",
    "longestmyvocabularies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempvocabulary=[]\n",
    "for k in range(len(paragraph_lemmatizedtexts)):\n",
    "    tempvocabulary.extend(myvocabularies[k])\n",
    "    \n",
    "# Find the unique elements among all vocabularies\n",
    "uniqueresults=np.unique(tempvocabulary,return_inverse=True)\n",
    "unifiedvocabulary=uniqueresults[0]\n",
    "wordindices=uniqueresults[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate previous indices to unified vocabulary.\n",
    "\n",
    "vocabularystart=0\n",
    "myindices_in_unifiedvocabulary=[]\n",
    "for k in range(len(paragraph_lemmatizedtexts)):\n",
    "    # In order to shift word indices, we must temporarily\n",
    "    # change their data type to a Numpy array\n",
    "    tempindices=np.array(myindices_in_vocabularies[k])\n",
    "    tempindices=tempindices+vocabularystart\n",
    "    tempindices=wordindices[tempindices]\n",
    "    myindices_in_unifiedvocabulary.append(tempindices)\n",
    "    vocabularystart=vocabularystart+len(myvocabularies[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "unifiedvocabulary_totaloccurrencecounts=np.zeros((len(unifiedvocabulary),1))\n",
    "unifiedvocabulary_documentcounts=np.zeros((len(unifiedvocabulary),1))\n",
    "unifiedvocabulary_meancounts=np.zeros((len(unifiedvocabulary),1))\n",
    "unifiedvocabulary_countvariances=np.zeros((len(unifiedvocabulary),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n"
     ]
    }
   ],
   "source": [
    "for k in range(len(longestpara)):\n",
    "    print(k)\n",
    "    occurrencecounts=np.zeros((len(unifiedvocabulary),1))\n",
    "    for l in range(len(myindices_in_unifiedvocabulary[k])):\n",
    "        occurrencecounts[myindices_in_unifiedvocabulary[k][l]]= \\\n",
    "            occurrencecounts[myindices_in_unifiedvocabulary[k][l]]+1\n",
    "    unifiedvocabulary_totaloccurrencecounts= \\\n",
    "        unifiedvocabulary_totaloccurrencecounts+occurrencecounts\n",
    "    unifiedvocabulary_documentcounts= \\\n",
    "        unifiedvocabulary_documentcounts+(occurrencecounts>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean occurrence counts over documents\n",
    "unifiedvocabulary_meancounts= \\\n",
    "    unifiedvocabulary_totaloccurrencecounts/len(longestpara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the' ',' '.' 'and' 'be' 'to' 'of' 'a' 'â\\x80\\x9d' 'in' 'have' 'i' 'he'\n",
      " 'you' 'her' 'she' 'they' 'it' 'that' 'say' 'dorothy' 'as' 'so' 'for'\n",
      " 'not' 'with' 'at' 'but' 'do' 'all' 'them' 'scarecrow' 'his' ';' '?' 'him'\n",
      " 'my' 'me' 'woodman' 'come' 'on' 'lion' 'oz' 'great' 'make' 'will' 'go'\n",
      " 'â\\x80\\x9ci' 'when' 'little' 'tin' 'witch' 'ask' 'this' 'one' 'could'\n",
      " 'from' 'see' 'would' 'then' 'there' 'we' 'if' 'who' 'get' 'green' 'out'\n",
      " 'up' 'can' 'their' 'head' 'look' 'know' 'no' 'think' 'girl' 'back' 'toto'\n",
      " '!' 'down' 'by' 'upon' 'answer' 'find' 'shall' 'again' 'city' 'give'\n",
      " 'into' 'over' 'very' 'must' 'wicked' 'now' 'emerald' 'man' 'good' 'where'\n",
      " 'after' 'walk']\n",
      "[2709. 2494. 1485. 1458. 1324. 1009.  760.  737.  633.  441.  424.  422.\n",
      "  410.  401.  369.  361.  357.  356.  342.  323.  321.  290.  282.  278.\n",
      "  256.  252.  237.  228.  217.  217.  212.  203.  198.  185.  178.  172.\n",
      "  172.  170.  168.  153.  151.  150.  148.  139.  139.  138.  137.  136.\n",
      "  136.  133.  133.  128.  126.  118.  114.  113.  111.  110.  110.  106.\n",
      "  105.  101.  100.   99.   98.   98.   95.   94.   92.   91.   90.   89.\n",
      "   89.   88.   87.   85.   84.   83.   83.   82.   80.   78.   78.   76.\n",
      "   76.   75.   73.   73.   73.   72.   72.   71.   71.   69.   67.   66.\n",
      "   66.   65.   65.   65.]\n"
     ]
    }
   ],
   "source": [
    "#%% Inspect frequent words\n",
    "# Sort words by largest total (or mean) occurrence count\n",
    "highest_totaloccurrences_indices=np.argsort(\\\n",
    "    -1*unifiedvocabulary_totaloccurrencecounts,axis=0)    \n",
    "print(np.squeeze(unifiedvocabulary[\\\n",
    "    highest_totaloccurrences_indices[0:100]]))\n",
    "print(np.squeeze(\\\n",
    "    unifiedvocabulary_totaloccurrencecounts[\\\n",
    "    highest_totaloccurrences_indices[0:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "#%% Vocabulary pruning\n",
    "nltkstopwords=nltk.corpus.stopwords.words('english')\n",
    "pruningdecisions=np.zeros((len(unifiedvocabulary),1))\n",
    "for k in range(len(unifiedvocabulary)):\n",
    "    # Rule 1: check the nltk stop word list\n",
    "    if (unifiedvocabulary[k] in nltkstopwords):\n",
    "        pruningdecisions[k]=1\n",
    "    # Rule 2: if the word is in the top 1% of frequent words\n",
    "    #if (k in highest_totaloccurrences_indices[\\\n",
    "    #    0:int(np.floor(len(unifiedvocabulary)*0.01))]):\n",
    "    #    pruningdecisions[k]=1\n",
    "   # Rule 3: if the word occurs less than 4 times\n",
    "    if(unifiedvocabulary_totaloccurrencecounts[k] < 4):\n",
    "        pruningdecisions[k] = 1\n",
    "    # Rule 4: if the word is too short\n",
    "    if len(unifiedvocabulary[k])<2:\n",
    "        pruningdecisions[k]=1\n",
    "    # Rule 5: if the word is too long\n",
    "    if len(unifiedvocabulary[k])>20:\n",
    "        pruningdecisions[k]=1\n",
    "    # Rule 6: if the word has unwanted characters\n",
    "    # (here for simplicity only a-z allowed)\n",
    "    if unifiedvocabulary[k].isalpha()==False:\n",
    "        pruningdecisions[k]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Get indices of documents to remaining words\n",
    "longestoldtopruned=[]\n",
    "tempind=-1\n",
    "for k in range(len(unifiedvocabulary)):\n",
    "    if pruningdecisions[k]==0:\n",
    "        tempind=tempind+1\n",
    "        longestoldtopruned.append(tempind)\n",
    "    else:\n",
    "        longestoldtopruned.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-5b02a3e883e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtemp_newdoc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlongestpara\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mtemp_oldindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmyindices_in_unifiedvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mtemp_newindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlongestoldtopruned\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtemp_oldindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtemp_newindex\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "#%% Create pruned texts\n",
    "longestparagraph_prunedtexts=[]\n",
    "longestmyindices_in_prunedvocabulary=[]\n",
    "for k in range(len(longestpara)):\n",
    "    print(k)\n",
    "    temp_newindices=[]\n",
    "    temp_newdoc=[]\n",
    "    for l in range(len(longestpara[k])):\n",
    "        temp_oldindex=myindices_in_unifiedvocabulary[k][l]\n",
    "        temp_newindex=longestoldtopruned[temp_oldindex]\n",
    "        if temp_newindex!=-1:\n",
    "            temp_newindices.append(temp_newindex)\n",
    "            temp_newdoc.append(unifiedvocabulary[temp_oldindex])\n",
    "    longestparagraph_prunedtexts.append(temp_newdoc)\n",
    "    longestmyindices_in_prunedvocabulary.append(temp_newindices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-54-aed8dc634baa>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-54-aed8dc634baa>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    print(np.squeeze(remainingvocabulary_totaloccurrencecounts[remaining_highest_totaloccurrences_indices[0:100]]\u001b[0m\n\u001b[1;37m                                                                                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "print('Top 100 word-list after pruning unified vocabulary:\\n')\n",
    "remaining_indices = np.squeeze(np.where(pruningdecisions==0)[0])\n",
    "remaining_vocabulary = unifiedvocabulary[remaining_indices]\n",
    "remainingvocabulary_totaloccurrencecounts = unifiedvocabulary_totaloccurrencecounts[remaining_indices]\n",
    "remaining_highest_totaloccurrences_indices = np.argsort(-1*remainingvocabulary_totaloccurrencecounts, axis=0)\n",
    "print(np.squeeze(remaining_vocabulary[remaining_highest_totaloccurrences_indices[0:100]]))\n",
    "print(np.squeeze(remainingvocabulary_totaloccurrencecounts[remaining_highest_totaloccurrences_indices[0:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs=len(longestparagraph_prunedtexts)\n",
    "n_vocab=len(remaining_vocabulary)\n",
    "# Matrix of term frequencies\n",
    "tfmatrix=scipy.sparse.lil_matrix((n_docs,n_vocab))\n",
    "# Row vector of document frequencies\n",
    "dfvector=scipy.sparse.lil_matrix((1,n_vocab))\n",
    "# Loop over documents\n",
    "for k in range(n_docs):\n",
    "    # Row vector of which words occurred in this document\n",
    "    temp_dfvector=scipy.sparse.lil_matrix((1,n_vocab))\n",
    "    # Loop over words\n",
    "    for l in range(len(paragraph_prunedtexts[k])):\n",
    "        # Add current word to term-frequency count and document-count\n",
    "        currentword=longestmyindices_in_prunedvocabulary[k][l]\n",
    "        tfmatrix[k,currentword]=tfmatrix[k,currentword]+1\n",
    "        temp_dfvector[0,currentword]=1\n",
    "    # Add which words occurred in this document to overall document counts\n",
    "    dfvector=dfvector+temp_dfvector\n",
    "\n",
    "# TF:length-normalized frequency\n",
    "for i in range(n_docs):\n",
    "    for j in range(len(tfmatrix.data[i])):\n",
    "        tfmatrix.data[i][j] = tfmatrix.data[i][j]/len(tfmatrix.data[i])\n",
    "        \n",
    "\n",
    "# smoothed logarithmic idf\n",
    "idfvector = np.squeeze(np.array(dfvector.todense()))\n",
    "idfvector = np.log(1 + ((idfvector+1)**-1)*n_docs)        \n",
    "\n",
    "tfidfmatrix = scipy.sparse.lil_matrix((n_docs, n_vocab))\n",
    "for k in range(n_docs):\n",
    "    # tf and idf terms\n",
    "    tfidfmatrix[k,:]=tfmatrix[k,:]*idfvector\n",
    "    \n",
    "# tf-idf matrix\n",
    "#tfidfmatrix = scipy.sparse.lil_matrix((n_docs, n_vocab))\n",
    "for k in range(n_docs):\n",
    "    # find nonzero term frequencies\n",
    "    tempindices = np.nonzero(tfmatrix[k, :])[1]\n",
    "    tfterm = np.squeeze(np.array(tfmatrix[k, tempindices].todense()))\n",
    "    tfidfmatrix[k, tempindices] = tfterm * idfvector[tempindices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Use the TF-IDF matrix as data to be clustered\n",
    "X=tfidfmatrix\n",
    "# Normalize the documents to unit vector norm\n",
    "tempnorms=np.squeeze(np.array(np.sum(X.multiply(X),axis=1)))\n",
    "# If any documents have zero norm, avoid dividing them by zero\n",
    "tempnorms[tempnorms==0]=1\n",
    "X=scipy.sparse.diags(tempnorms**-0.5).dot(X)\n",
    "n_data=np.shape(X)[0]\n",
    "n_dimensions=np.shape(X)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize the Gaussian mixture model, create component parameters\n",
    "def initialize_mixturemodel(X,n_components):\n",
    "    # Create lists of sparse matrices to hold the parameters\n",
    "    n_dimensions=np.shape(X)[1]\n",
    "    n_data = np.shape(X)[0]\n",
    "    mixturemodel_means=scipy.sparse.lil_matrix((n_components,n_dimensions))\n",
    "    mixturemodel_weights=np.zeros((n_components))\n",
    "    mixturemodel_covariances=[]\n",
    "    mixturemodel_inversecovariances=[]\n",
    "    for k in range(n_components):\n",
    "        tempcovariance=scipy.sparse.lil_matrix((n_dimensions,n_dimensions))\n",
    "        mixturemodel_covariances.append(tempcovariance)\n",
    "        tempinvcovariance=scipy.sparse.lil_matrix((n_dimensions,n_dimensions))\n",
    "        mixturemodel_inversecovariances.append(tempinvcovariance)\n",
    "    # Initialize the parameters\n",
    "    for k in range(n_components):\n",
    "        mixturemodel_weights[k]=1/n_components\n",
    "        # Pick a random data point as the initial mean\n",
    "        tempindex=scipy.stats.randint.rvs(low=0,high=n_data)\n",
    "        mixturemodel_means[k]=X[tempindex,:].toarray()\n",
    "        # Initialize the covariance matrix to be spherical\n",
    "        for l in range(n_dimensions):\n",
    "            mixturemodel_covariances[k][l,l]=1\n",
    "            mixturemodel_inversecovariances[k][l,l]=1\n",
    "    return(mixturemodel_weights,mixturemodel_means,mixturemodel_covariances,mixturemodel_inversecovariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_estep(X,mixturemodel_means,mixturemodel_covariances,mixturemodel_inversecovariances,mixturemodel_weights):\n",
    "    # For each component, compute terms that do not involve data\n",
    "    meanterms=np.zeros((n_components))\n",
    "    logdeterminants=np.zeros((n_components))\n",
    "    logconstantterms=np.zeros((n_components))\n",
    "    \n",
    "    for k in range(n_components):\n",
    "        # Compute mu_k*inv(Sigma_k)*mu_k\n",
    "        meanterms[k]=(mixturemodel_means[k,:]*mixturemodel_inversecovariances[k]*mixturemodel_means[k,:].T)[0,0]\n",
    "        # Compute determinant of Sigma_k. For a diagonal matrix\n",
    "        # this is just the product of the main diagonal\n",
    "        logdeterminants[k]=np.sum(np.log(mixturemodel_covariances[k].diagonal(0)))\n",
    "        # Compute constant term beta_k * 1/(|Sigma_k|^1/2)\n",
    "        # Omit the (2pi)^d/2 as it cancels out\n",
    "        logconstantterms[k]=np.log(mixturemodel_weights[k]) - 0.5*logdeterminants[k]\n",
    "    \n",
    "    print('E-step part2 ')\n",
    "    # Compute terms that involve distances of data from components\n",
    "    xnorms=np.zeros((n_data,n_components))\n",
    "    xtimesmu=np.zeros((n_data,n_components))\n",
    "    \n",
    "    for k in range(n_components):\n",
    "        #print(k)\n",
    "        xnorms[:,k]=(X*mixturemodel_inversecovariances[k]*X.T).diagonal(0)\n",
    "        xtimesmu[:,k]=np.squeeze((X*mixturemodel_inversecovariances[k]* mixturemodel_means[k,:].T).toarray())\n",
    "        \n",
    "    xdists=xnorms+np.matlib.repmat(meanterms,n_data,1)-2*xtimesmu\n",
    "    # Substract maximal term before exponent (cancels out) to maintain computational precision\n",
    "    numeratorterms=logconstantterms-xdists/2\n",
    "    numeratorterms-=np.matlib.repmat(np.max(numeratorterms,axis=1),n_components,1).T\n",
    "    numeratorterms=np.exp(numeratorterms)\n",
    "    mixturemodel_componentmemberships=numeratorterms/np.matlib.repmat(np.sum(numeratorterms,axis=1),n_components,1).T\n",
    "    return(mixturemodel_componentmemberships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mstep_sumweights(mixturemodel_componentmemberships):\n",
    "    # Compute total weight per component\n",
    "    mixturemodel_weights=np.sum(mixturemodel_componentmemberships,axis=0)\n",
    "    return(mixturemodel_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mstep_means(X,mixturemodel_componentmemberships,mixturemodel_weights):\n",
    "    # Update component means\n",
    "    mixturemodel_means=scipy.sparse.lil_matrix((n_components,n_dimensions))\n",
    "    for k in range(n_components):\n",
    "        mixturemodel_means[k,:]=\\\n",
    "            np.sum(scipy.sparse.diags(mixturemodel_componentmemberships[:,k]).dot(X),axis=0)\n",
    "        mixturemodel_means[k,:]/=mixturemodel_weights[k]\n",
    "    return(mixturemodel_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mstep_covariances(X,mixturemodel_componentmemberships,mixturemodel_weights,mixturemodel_means):\n",
    "    # Update diagonal component covariance matrices\n",
    "    n_dimensions=np.shape(X)[1]\n",
    "    n_components=np.shape(mixturemodel_componentmemberships)[1]\n",
    "    tempcovariances=np.zeros((n_components,n_dimensions))\n",
    "    mixturemodel_covariances=[]\n",
    "    mixturemodel_inversecovariances=[]\n",
    "    \n",
    "    for k in range(n_components):\n",
    "        tempcovariances[k,:]= np.sum(scipy.sparse.diags(\n",
    "            mixturemodel_componentmemberships[:,k]).dot(\n",
    "            X.multiply(X)),axis=0)-mixturemodel_means[k,:].multiply(mixturemodel_means[k,:])*mixturemodel_weights[k]\n",
    "        tempcovariances[k,:]/=mixturemodel_weights[k]\n",
    "        # Convert to sparse matrices\n",
    "        tempepsilon=1e-10\n",
    "        # Add a small regularization term\n",
    "        temp_covariance=scipy.sparse.diags(tempcovariances[k,:]+tempepsilon)\n",
    "        temp_inversecovariance=scipy.sparse.diags((tempcovariances[k,:]+tempepsilon)**-1)\n",
    "        mixturemodel_covariances.append(temp_covariance)\n",
    "        mixturemodel_inversecovariances.append(temp_inversecovariance)\n",
    "    return(mixturemodel_covariances,mixturemodel_inversecovariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mstep_normalizeweights(mixturemodel_weights):\n",
    "    # Update mixture-component prior probabilities\n",
    "    mixturemodel_weights/=sum(mixturemodel_weights)\n",
    "    return(mixturemodel_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Domain error in arguments.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-b8e189326e21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mn_emiterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mmixturemodel_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmixturemodel_means\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmixturemodel_covariances\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmixturemodel_inversecovariances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperform_emalgorithm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_emiterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-63-b8e189326e21>\u001b[0m in \u001b[0;36mperform_emalgorithm\u001b[1;34m(X, n_components, n_emiterations)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#%% Perform the EM algorithm iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mperform_emalgorithm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_emiterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmixturemodel_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmixturemodel_means\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmixturemodel_covariances\u001b[0m\u001b[1;33m,\u001b[0m    \u001b[0mmixturemodel_inversecovariances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitialize_mixturemodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_emiterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m# ====== E-step: Compute the component membership\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-8993e11d61c8>\u001b[0m in \u001b[0;36minitialize_mixturemodel\u001b[1;34m(X, n_components)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mmixturemodel_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Pick a random data point as the initial mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mtempindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrvs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhigh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mmixturemodel_means\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtempindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Initialize the covariance matrix to be spherical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36mrvs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2807\u001b[0m         \"\"\"\n\u001b[0;32m   2808\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'discrete'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2809\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrv_discrete\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrvs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2811\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpmf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36mrvs\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    938\u001b[0m         \u001b[0mcond\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogical_and\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_argcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscale\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Domain error in arguments.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Domain error in arguments."
     ]
    }
   ],
   "source": [
    "#%% Perform the EM algorithm iterations\n",
    "def perform_emalgorithm(X,n_components,n_emiterations):\n",
    "    mixturemodel_weights,mixturemodel_means,mixturemodel_covariances,\\\n",
    "    mixturemodel_inversecovariances=initialize_mixturemodel(X,n_components)\n",
    "    for t in range(n_emiterations):\n",
    "        # ====== E-step: Compute the component membership\n",
    "        # probabilities of each data point ======\n",
    "        print('E-step ' + str(t))\n",
    "        mixturemodel_componentmemberships=run_estep(X,mixturemodel_means,mixturemodel_covariances,\\\n",
    "        mixturemodel_inversecovariances,mixturemodel_weights)\n",
    "        # ====== M-step: update component parameters======\n",
    "        print('M-step ' + str(t))\n",
    "        print('M-step part1 ' + str(t))\n",
    "        mixturemodel_weights=run_mstep_sumweights(mixturemodel_componentmemberships)\n",
    "        print('M-step part2 ' + str(t))\n",
    "        mixturemodel_means=run_mstep_means(X,mixturemodel_componentmemberships,mixturemodel_weights)\n",
    "        print('M-step part3 ' + str(t))\n",
    "        mixturemodel_covariances,mixturemodel_inversecovariances=run_mstep_covariances(X,\\\n",
    "        mixturemodel_componentmemberships,mixturemodel_weights,mixturemodel_means)\n",
    "        print('M-step part4 ' + str(t))\n",
    "        mixturemodel_weights=run_mstep_normalizeweights(mixturemodel_weights)\n",
    "    return(mixturemodel_weights,mixturemodel_means,mixturemodel_covariances,\\\n",
    "mixturemodel_inversecovariances)\n",
    "# Try out the functions we just defined on the data\n",
    "n_components=10\n",
    "n_emiterations=100\n",
    "mixturemodel_weights,mixturemodel_means,mixturemodel_covariances,\\\n",
    "mixturemodel_inversecovariances = perform_emalgorithm(X,n_components,n_emiterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "look supper lonely suppose sure surely listen surprise surprised\n",
      "1\n",
      "lucky magic tinsmith man manage mane low many mark\n",
      "2\n",
      "pat pave perch person pick piece pin pass place\n",
      "3\n",
      "pole fairy prairie presently pull everywhere purple everyone push\n",
      "4\n",
      "inside inquire sweep immediately imagine swiftly table hour hot\n",
      "5\n",
      "others ought palace part party pat patch order pave\n",
      "6\n",
      "hung humbug however story hour hot horse hope home\n",
      "7\n",
      "oblige offer often oil old open order ought paint\n",
      "8\n",
      "marry search ruby careful ought name joyfully chapter sadly\n",
      "9\n",
      "fairy nearly whirl explain expect exclaim need move exactly\n"
     ]
    }
   ],
   "source": [
    "for k in range(n_components):\n",
    "    print(k)\n",
    "    highest_dimensionweight_indices=np.argsort(-np.squeeze(mixturemodel_means[k,:].toarray()),axis=0)\n",
    "\n",
    "    print(' '.join(remaining_vocabulary[highest_dimensionweight_indices[1:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "k exceeds matrix dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-a71fcfb83997>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mxnorms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmixturemodel_inversecovariances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mxtimesmu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmixturemodel_inversecovariances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmixturemodel_means\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mdiagonal\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mrows\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"k exceeds matrix dimensions\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m         \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sparsetools\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_diagonal\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m         y = np.empty(min(rows + min(k, 0), cols - max(k, 0)),\n",
      "\u001b[1;31mValueError\u001b[0m: k exceeds matrix dimensions"
     ]
    }
   ],
   "source": [
    "# Version 2 - Get documents closest to component mean, i.e. highest p(d|k).\n",
    "# ---The computation of distances here is the same as done in the E-step of EM---\n",
    "# For each component, compute terms that do not involve data\n",
    "meanterms=np.zeros((n_components))\n",
    "logdeterminants=np.zeros((n_components))\n",
    "logconstantterms=np.zeros((n_components))\n",
    "for k in range(n_components):\n",
    "    # Compute mu_k*inv(Sigma_k)*mu_k\n",
    "    meanterms[k]=(mixturemodel_means[k,:]*mixturemodel_inversecovariances[k]*mixturemodel_means[k,:].T)[0,0]\n",
    "\n",
    "# Compute terms that involve distances of data from components\n",
    "xnorms=np.zeros((n_data,n_components))\n",
    "xtimesmu=np.zeros((n_data,n_components))\n",
    "\n",
    "for k in range(n_components):\n",
    "    xnorms[:,k]=(X*mixturemodel_inversecovariances[k]*X.T).diagonal(0)\n",
    "    xtimesmu[:,k]=np.squeeze((X*mixturemodel_inversecovariances[k]*mixturemodel_means[k,:].T).toarray())\n",
    "\n",
    "xdists=xnorms+np.matlib.repmat(meanterms,n_data,1)-2*xtimesmu\n",
    "\n",
    "for k in range(n_components):\n",
    "    tempdists=np.array(np.squeeze(xdists[:,k]))\n",
    "    highest_componentprob_indices=np.argsort(-1*tempdists,axis=0)\n",
    "    print(k)\n",
    "    print(highest_componentprob_indices[0:10])\n",
    "    print(' '.join(paragraph_nltk_texts[highest_componentprob_indices[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
