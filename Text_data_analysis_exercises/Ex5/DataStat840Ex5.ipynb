{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA.STAT.840 Statistical Methods for Text Data Analysis\n",
    "Exercises for Lecture 5: N-grams\n",
    "Daniel Kusnetsoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.3: More adventures of Robin Hood, and a new journey to Mars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import nltk\n",
    "import numpy as np\n",
    "#from nltk import word_tokenize, sent_tokenize\n",
    "#import nltk.lm\n",
    "nltk.download('nltk.lm')\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "import nltk.lm\n",
    "#from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Get the text content of the page\n",
    "def getpagetext(parsedpage):\n",
    "    # Remove HTML elements that are scripts\n",
    "    scriptelements=parsedpage.find_all('script')\n",
    "    # Concatenate the text content from all table cells\n",
    "    for scriptelement in scriptelements:\n",
    "        # Extract this script element from the page.\n",
    "        # This changes the page given to this function!\n",
    "        scriptelement.extract()\n",
    "    pagetext=parsedpage.get_text()\n",
    "    return(pagetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def download_specific_ebook(ebook_url):\n",
    "    ebook_page = requests.get(ebook_url)\n",
    "    parsed_page = bs4.BeautifulSoup(ebook_page.content, 'html.parser')\n",
    "    ebook_text = getpagetext(parsed_page)\n",
    "    start_text = '*** START OF THIS PROJECT GUTENBERG***'\n",
    "    start_index = ebook_text.find(start_text)\n",
    "    end_index = ebook_text.find('*** END OF THE PROJECT GUTENBERG EBOOK')\n",
    "    ebook_text = ebook_text[start_index + len(start_text):end_index]\n",
    "    \n",
    "    # remove whitespaces\n",
    "    ebook_text = ebook_text.strip()\n",
    "    ebook_text = ' '.join(ebook_text.split())\n",
    "    return(ebook_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robinHood_text = download_specific_ebook('https://www.gutenberg.org/files/10148/10148.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "martianOdyssey_text = download_specific_ebook('https://www.gutenberg.org/files/23731/23731.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "robinHood_tokenized_text = nltk.word_tokenize(robinHood_text)\n",
    "# NLTK-format text\n",
    "robinHood_nltk_texts = nltk.Text(robinHood_tokenized_text)\n",
    "# lowercase the text \n",
    "robinHood_lowercase_texts = []\n",
    "for l in range(len(robinHood_nltk_texts)):\n",
    "    lowercase_word = robinHood_nltk_texts[l].lower()\n",
    "    robinHood_lowercase_texts.append(lowercase_word)\n",
    "robinHood_tokenized_text=robinHood_lowercase_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robinHood_tokenized_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "martianOdyssey_tokenized_text = nltk.word_tokenize(martianOdyssey_text)\n",
    "# NLTK-format text\n",
    "martianOdyssey_nltk_texts = nltk.Text(martianOdyssey_tokenized_text)\n",
    "# lowercase the text \n",
    "martianOdyssey_lowercase_texts = []\n",
    "for l in range(len(martianOdyssey_nltk_texts)):\n",
    "    lowercase_word = martianOdyssey_nltk_texts[l].lower()\n",
    "    martianOdyssey_lowercase_texts.append(lowercase_word)\n",
    "martianOdyssey_tokenized_text=martianOdyssey_lowercase_texts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "martianOdyssey_tokenized_text[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Find the vocabulary, in a distributed fashion\n",
    "robinHood_vocabularies=[]\n",
    "robinHood_indices_in_vocabularies=[]\n",
    "# Find the vocabulary of each document\n",
    "for k in range(len(robinHood_tokenized_text)):\n",
    "    # Get unique words and where they occur\n",
    "    temptext=robinHood_tokenized_text[k]\n",
    "    uniqueresults=np.unique(temptext,return_inverse=True)\n",
    "    uniquewords=uniqueresults[0]\n",
    "    wordindices=uniqueresults[1]\n",
    "    # Store the vocabulary and indices of document words in it\n",
    "    robinHood_vocabularies.append(uniquewords)\n",
    "    robinHood_indices_in_vocabularies.append(wordindices)\n",
    "robinHood_vocabularies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robinHood_vocabularies[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Find the vocabulary, in a distributed fashion\n",
    "martianOdyssey_vocabularies=[]\n",
    "martianOdyssey_indices_in_vocabularies=[]\n",
    "# Find the vocabulary of each document\n",
    "for k in range(len(martianOdyssey_tokenized_text)):\n",
    "    # Get unique words and where they occur\n",
    "    temptext=martianOdyssey_tokenized_text[k]\n",
    "    uniqueresults=np.unique(temptext,return_inverse=True)\n",
    "    uniquewords=uniqueresults[0]\n",
    "    wordindices=uniqueresults[1]\n",
    "    # Store the vocabulary and indices of document words in it\n",
    "    martianOdyssey_vocabularies.append(uniquewords)\n",
    "    martianOdyssey_indices_in_vocabularies.append(wordindices)\n",
    "martianOdyssey_vocabularies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "martianOdyssey_vocabularies[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk.lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_model(maxN, robinHood_tokenized_text):\n",
    "    # Create N-gram training data\n",
    "    ngramtraining_data, added_sentences = nltk.lm.preprocessing.padded_everygram_pipeline(maxN, robinHood_tokenized_text)\n",
    "    # Create the maximum-likelihood n-gram estimate\n",
    "    ngrammodel = nltk.lm.MLE(maxN)\n",
    "    ngrammodel.fit(ngramtraining_data, padded_sentences)\n",
    "    return(ngrammodel)\n",
    "robinHood_tokenized_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "#detokenize = TreebankWordDetokenizer().detokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def para_ngram(ngram_model, n_words):\n",
    "    content = []\n",
    "    for token in ngram_model.generate(n_words):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def build_ngram(textindexvector,n_vocab,maxN):\n",
    "    # Create the overall structure that will store the n-gram\n",
    "    allprobstructs_NextCount=[]\n",
    "    allprobstructs_NextProb=[]\n",
    "    allprobstructs_PreviousStruct=[]\n",
    "    # Create unigram probability table, store it as the root of probtables\n",
    "    tempstruct_NextCount=scipy.sparse.dok_matrix((n_vocab,1))\n",
    "    tempstruct_NextProb=scipy.sparse.dok_matrix((n_vocab,1))\n",
    "    tempstruct_PreviousStruct=scipy.sparse.dok_matrix((n_vocab,1))\n",
    "    allprobstructs_NextCount.append(tempstruct_NextCount)\n",
    "    allprobstructs_NextProb.append(tempstruct_NextProb)\n",
    "    allprobstructs_PreviousStruct.append(tempstruct_PreviousStruct)\n",
    "    nstructs=1\n",
    "    # Count how many probability tables have been created at different\n",
    "    # n-gram levels. Because this is a zero-based index, the index of the\n",
    "    # level indicating how long a history each n-gram takes into account:\n",
    "    # 0 for unigrams, 1 for bigrams, and so on.\n",
    "    nstructsperlevel=np.zeros((maxN))\n",
    "    # Initially there is only one table which is a unigram-table.\n",
    "    nstructsperlevel[0]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through the text\n",
    "for t in range(len(textindexvector)):\n",
    "    if (t%10)==0:\n",
    "        print(str(t) + ' ' + str(nstructsperlevel))\n",
    "    # Vocabulary index of the word at position t in the text\n",
    "    tempword=textindexvector[t-0];\n",
    "    # Suppose we have words w(1),...,w(t-3),w(t-2),w(t-1),w(t) in the text.\n",
    "    # Then the transition to w(t) must be recorded for several n-grams:\n",
    "    # []-->w(t) : unigram transition (n=1)\n",
    "    # [w(t-1)]-->w(t) : bigram transition (n=2)\n",
    "    # [w(t-2),w(t-1)]-->w(t) : trigram transition (n=3)\n",
    "    # [w(t-3),w(t-2),w(t-1)]-->w(t) : 4-gram transition (n=4)\n",
    "    # [w(t-4),w(t-3),w(t-2),w(t-1)]-->w(t) : 5-gram transition (n=5)\n",
    "    # Start from the unigram (root of the tables), record the transition\n",
    "    currentstruct=0\n",
    "    # Record the transition into the transition counts:\n",
    "    # and its count in the unigram model increases by 1.\n",
    "    #allprobstructs[currentstruct]['Next'][tempword,0]=1\n",
    "    allprobstructs_NextCount[currentstruct][tempword,0]+=1\n",
    "    # Now record this transition into higher-level n-grams.\n",
    "    # Address history up to maximum N-gram length or beginning of\n",
    "    # the text, whichever is sooner.\n",
    "    # Iterate a zero-based index \"n\" of steps back\n",
    "    for n in range(min([maxN-1,t])):\n",
    "        # Take the next step back.\n",
    "        # Vocabulary index of the previous word\n",
    "        previousword=textindexvector[t-n-1];\n",
    "        # At this point in the for loop, the current probability table\n",
    "        # allprobstructs[currentstruct] represents a (n+1)-gram which uses\n",
    "        # a history of length (n): \"[w(t-n),...,w(t-1)]--->NextWord\".\n",
    "        # The \"previousword\" w(t-n-1) is an expansion of it into a\n",
    "        # (n+2)-gram which uses a history of length (n+1):\n",
    "        # \"[w(t-n-1),...,w(t-1)]--->NextWord\". This expansion might exist\n",
    "        # already or not. The field 'Previous' of the current (n+1)-gram\n",
    "        # records whether that expansion exists.\n",
    "        # Create a new history reference (next-level ngram) if it\n",
    "        # did not exist.\n",
    "        # Note that the unigram table has index 0, but it is never an\n",
    "        # expansion of a smaller n-gram.\n",
    "        if allprobstructs_PreviousStruct[currentstruct][previousword,0]==0:\n",
    "            # Create the probability table for the expansion. Because this\n",
    "            # is the first time this \"[w(t-n-1),...,w(t-1)]--->NextWord\" has\n",
    "            # been needed, initially it has no next-words (the current\n",
    "            # word will become its first observed next-word). Similarly,\n",
    "            # because this is the first time this table is needed, it cannot\n",
    "            # have any higher-level expansions yet.\n",
    "            tempstruct_NextCount=scipy.sparse.dok_matrix((n_vocab,1));\n",
    "            tempstruct_NextProb=scipy.sparse.dok_matrix((n_vocab,1));\n",
    "            tempstruct_PreviousStruct=scipy.sparse.dok_matrix((n_vocab,1));\n",
    "            # Add the created table into the overall list of all probability\n",
    "            # tables, increase the count of tables overall and at the n-gram\n",
    "            # level (history length n+1) where the table was created.\n",
    "            nstructs+=1;\n",
    "            nstructsperlevel[n+1]+=1;\n",
    "            allprobstructs_NextCount.append(tempstruct_NextCount)\n",
    "            allprobstructs_NextProb.append(tempstruct_NextProb)\n",
    "            allprobstructs_PreviousStruct.append(tempstruct_PreviousStruct)\n",
    "            # Mark that the expansion now exists into the current\n",
    "            # current \"[w(t-n),...,w(t-1)]--->NextWord\" table\n",
    "            # allprobstructs[currentstruct]['Previous'][previousword,0]=1\n",
    "            # Add a pointer from the current table to the newly\n",
    "            # created structure (index of the newly created table in the\n",
    "            # overall list)\n",
    "            allprobstructs_PreviousStruct \\\n",
    "            [currentstruct][previousword,0]=nstructs-1;\n",
    "            # At this point we can be sure the next-level n-gram exists, so we\n",
    "            # go to the next-level ngram and add the newest word-occurrence to it\n",
    "            # as a possible next word, increasing its count.\n",
    "        currentstruct=allprobstructs_PreviousStruct \\\n",
    "        [currentstruct][previousword,0];\n",
    "        currentstruct=int(currentstruct)\n",
    "        allprobstructs_NextCount[currentstruct][tempword,0]+=1\n",
    "# For all tables that have been created, obtain their probabilities by\n",
    "# normalizing their counts\n",
    "for k in range(nstructs):\n",
    "    allprobstructs_NextProb[k]=allprobstructs_NextCount[k] \\\n",
    "    /numpy.sum(allprobstructs_NextCount[k]);\n",
    "return((allprobstructs_NextCount,allprobstructs_NextProb,allprobstructs_PreviousStruct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temptext=' '.join(robinHood_tokenized_text)\n",
    "temptext=list(temptext)\n",
    "# Create the \"vocabulary\" of the different individual characters\n",
    "tempvocabulary=[]\n",
    "myindices_in_tempvocabulary=[]\n",
    "# Find the vocabulary of each document.\n",
    "# Get unique words and where they occur\n",
    "uniqueresults=np.unique(temptext,return_inverse=True)\n",
    "tempvocabulary=uniqueresults[0]\n",
    "tempindices=uniqueresults[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temptext[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import scipy.stats\n",
    "def sample_ngram(allprobstructs,n_words_to_sample,maxN,initialtext):\n",
    "    allprobstructs_NextProb=allprobstructs[1]\n",
    "    allprobstructs_PreviousStruct=allprobstructs[2]\n",
    "    sampletext=[]\n",
    "    sampletext.extend(initialtext)\n",
    "    for k in range(n_words_to_sample):\n",
    "        # We are sampling a new word for position t\n",
    "        t=len(initialtext)+k\n",
    "        # Start from unigram probability table\n",
    "        currentstruct=0\n",
    "        # Try to use as much history as possible for sampling the next\n",
    "        # word, but revert to smaller n-gram if data is not available for\n",
    "        # the current history\n",
    "        historycount=0\n",
    "        for n in range(min([maxN-1,t])):\n",
    "            # If we want, we can set a probability to use a higher-level n-gram\n",
    "            usehigherlevel_probability=0.99\n",
    "            if (scipy.stats.uniform.rvs() < usehigherlevel_probability):\n",
    "                # Try to advance to the next-level n-gram\n",
    "                historycount=historycount+1\n",
    "                #print((t,historycount,len(sampletext)))\n",
    "                previousword=sampletext[t-historycount]\n",
    "                if allprobstructs_PreviousStruct[currentstruct][previousword,0]>0:\n",
    "                    currentstruct=allprobstructs_PreviousStruct[currentstruct][previousword,0]\n",
    "                    currentstruct=int(currentstruct)\n",
    "                else:\n",
    "                    # Don't try to advance any more times, just exist the for-loop\n",
    "                    break\n",
    "        # At this point we have found a probability table at some history level.\n",
    "        # Sample from its nonzero entries.\n",
    "        possiblewords=allprobstructs_NextProb[currentstruct].nonzero()[0]\n",
    "        possibleprobs=numpy.squeeze(allprobstructs_NextProb[currentstruct][possiblewords,0].toarray(),axis=1)\n",
    "        currentword=numpy.random.choice(possiblewords, p=possibleprobs)\n",
    "        sampletext.append(currentword)\n",
    "        # Return the created text\n",
    "    return(sampletext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the n-gram\n",
    "maxN=1\n",
    "myngram=n_gram_model(maxN, robinHood_tokenized_text)\n",
    "# sample from the result\n",
    "n_words_to_sample=800\n",
    "initialtext=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampledtext=sample_ngram(myngram,n_words_to_sample,maxN,initialtext)\n",
    "''.join(tempvocabulary[sampledtext])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxN=5\n",
    "myngram=build_ngram(myindices_in_tempvocabulary[0],len(tempvocabulary[0]),maxN)\n",
    "# This can be an array of vocabulary indices of previously observed words\n",
    "initialtext=[]\n",
    "# Sample a vector of word indices from the 5-gram\n",
    "# following the initial text\n",
    "n_words_to_sample=100\n",
    "sampledtext=sample_ngram(myngram,n_words_to_sample,maxN,initialtext)\n",
    "# Print the result\n",
    "' '.join(myvocabularies[0][sampledtext])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
