{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kklfLffio38a"
      },
      "source": [
        "# DATA.STAT.840 Statistical Methods for Text Data Analysis\n",
        "Exercises for Lecture 5: N-grams\n",
        "Daniel Kusnetsoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdqIZFFao38f"
      },
      "source": [
        "# Exercise 5.3: More adventures of Robin Hood, and a new journey to Mars."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL93fzQBo38g",
        "outputId": "4734ef9f-eef4-49c3-d82f-e47377aba1a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Error loading nltk.lm: Package 'nltk.lm' not found in\n",
            "[nltk_data]     index\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import requests\n",
        "import bs4\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('nltk.lm')\n",
        "\n",
        "from nltk.util import ngrams\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u48ds9ejo38h"
      },
      "outputs": [],
      "source": [
        "#%% Get the text content of the page\n",
        "def getpagetext(parsedpage):\n",
        "    # Remove HTML elements that are scripts\n",
        "    scriptelements=parsedpage.find_all('script')\n",
        "    # Concatenate the text content from all table cells\n",
        "    for scriptelement in scriptelements:\n",
        "        # Extract this script element from the page.\n",
        "        # This changes the page given to this function!\n",
        "        scriptelement.extract()\n",
        "    pagetext=parsedpage.get_text()\n",
        "    return(pagetext)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CHEEe-q2o38i"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "\n",
        "def download_specific_ebook(ebook_url):\n",
        "    ebook_page = requests.get(ebook_url)\n",
        "    parsed_page = bs4.BeautifulSoup(ebook_page.content, 'html.parser')\n",
        "    ebook_text = getpagetext(parsed_page)\n",
        "    start_text = '*** START OF THIS PROJECT GUTENBERG***'\n",
        "    start_index = ebook_text.find(start_text)\n",
        "    end_index = ebook_text.find('*** END OF THE PROJECT GUTENBERG EBOOK')\n",
        "    ebook_text = ebook_text[start_index + len(start_text):end_index]\n",
        "    \n",
        "    # remove whitespaces\n",
        "    ebook_text = ebook_text.strip()\n",
        "    ebook_text = ' '.join(ebook_text.split())\n",
        "    return(ebook_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0mIcxD32o38j"
      },
      "outputs": [],
      "source": [
        "robinHood_text = download_specific_ebook('https://www.gutenberg.org/files/10148/10148.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-49iBgIDo38k"
      },
      "outputs": [],
      "source": [
        "martianOdyssey_text = download_specific_ebook('https://www.gutenberg.org/files/23731/23731.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pHeVfXPdo38l"
      },
      "outputs": [],
      "source": [
        "import nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_1xewKzco38l"
      },
      "outputs": [],
      "source": [
        "# tokenize text\n",
        "robinHood_tokenized_text = nltk.word_tokenize(robinHood_text)\n",
        "# NLTK-format text\n",
        "robinHood_nltk_texts = nltk.Text(robinHood_tokenized_text)\n",
        "# lowercase the text \n",
        "robinHood_lowercase_texts = []\n",
        "for l in range(len(robinHood_nltk_texts)):\n",
        "    lowercase_word = robinHood_nltk_texts[l].lower()\n",
        "    robinHood_lowercase_texts.append(lowercase_word)\n",
        "robinHood_tokenized_text=robinHood_lowercase_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vD_hRAEMo38n"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "robinHood_tokenized_text= [list(map(str.lower, word_tokenize(sent))) \n",
        "                                 for sent in sent_tokenize(robinHood_text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nZyv9Oeho38n"
      },
      "outputs": [],
      "source": [
        "# tokenize text\n",
        "martianOdyssey_tokenized_text = nltk.word_tokenize(martianOdyssey_text)\n",
        "# NLTK-format text\n",
        "martianOdyssey_nltk_texts = nltk.Text(martianOdyssey_tokenized_text)\n",
        "# lowercase the text \n",
        "martianOdyssey_lowercase_texts = []\n",
        "for l in range(len(martianOdyssey_nltk_texts)):\n",
        "    lowercase_word = martianOdyssey_nltk_texts[l].lower()\n",
        "    martianOdyssey_lowercase_texts.append(lowercase_word)\n",
        "martianOdyssey_tokenized_text=martianOdyssey_lowercase_texts    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P8EvbTmao38o"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "martianOdyssey_tokenized_text= [list(map(str.lower, word_tokenize(sent))) \n",
        "                                 for sent in sent_tokenize(martianOdyssey_text)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "martianOdyssey_tokenized_text[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkNkFPqnTr_L",
        "outputId": "d96a4add-5d65-4f5c-82ba-01697bd35303"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ian',\n",
              " 'odyssey',\n",
              " ',',\n",
              " 'by',\n",
              " 'stanley',\n",
              " 'grauman',\n",
              " 'weinbaum',\n",
              " 'this',\n",
              " 'ebook',\n",
              " 'is',\n",
              " 'for',\n",
              " 'the',\n",
              " 'use',\n",
              " 'of',\n",
              " 'anyone',\n",
              " 'anywhere',\n",
              " 'at',\n",
              " 'no',\n",
              " 'cost',\n",
              " 'and',\n",
              " 'with',\n",
              " 'almost',\n",
              " 'no',\n",
              " 'restrictions',\n",
              " 'whatsoever',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oEh2g-A7TsCI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0HnoB9Go38o",
        "outputId": "85aaa228-b3dc-4588-8b47-b12130ff3d68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([',', '.', 'almost', 'and', 'anyone', 'anywhere', 'at', 'by',\n",
              "       'cost', 'ebook', 'for', 'hood', 'howard', 'is', 'no', 'of', 'pyle',\n",
              "       'res', 'restrictions', 'robin', 'the', 'this', 'use', 'whatsoever',\n",
              "       'with'], dtype='<U12')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#%% Find the vocabulary, in a distributed fashion\n",
        "robinHood_vocabularies=[]\n",
        "robinHood_indices_in_vocabularies=[]\n",
        "# Find the vocabulary of each document\n",
        "for k in range(len(robinHood_tokenized_text)):\n",
        "    # Get unique words and where they occur\n",
        "    temptext=robinHood_tokenized_text[k]\n",
        "    uniqueresults=np.unique(temptext,return_inverse=True)\n",
        "    uniquewords=uniqueresults[0]\n",
        "    wordindices=uniqueresults[1]\n",
        "    # Store the vocabulary and indices of document words in it\n",
        "    robinHood_vocabularies.append(uniquewords)\n",
        "    robinHood_indices_in_vocabularies.append(wordindices)\n",
        "robinHood_vocabularies[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKN7vVjDo38p",
        "outputId": "bb1823df-34a2-47bb-995f-3ee32b3a4bc3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([',', '.', 'almost', 'and', 'anyone', 'anywhere', 'at', 'by',\n",
              "        'cost', 'ebook', 'for', 'hood', 'howard', 'is', 'no', 'of', 'pyle',\n",
              "        'res', 'restrictions', 'robin', 'the', 'this', 'use', 'whatsoever',\n",
              "        'with'], dtype='<U12'),\n",
              " array(['#', '*', ',', '.', '10148', '20', '2003', ':', ';', '[', ']', 'a',\n",
              "        'adventures', 'amid', 'and', 'are', 'ascii', 'at', 'author',\n",
              "        'away', 'by', 'can', 'character', 'copy', 'date', 'david',\n",
              "        'distributed', 'do', 'ebook', 'encoding', 'english', 'even',\n",
              "        'fancy', 'feel', 'few', 'for', 'from', 'garvin', 'give',\n",
              "        'gutenberg', 'harm', 'hath', 'hood', 'howard', 'in', 'included',\n",
              "        'innocent', 'it', 'joyousness', 'land', 'language', 'laughter',\n",
              "        'license', 'life', 'may', 'merry', 'mirth', 'moments', 'no', 'not',\n",
              "        'nought', 'november', 'of', 'one', 'online', 'or', 'pages', 'pg',\n",
              "        'plod', 'preface', 'produced', 'project', 'proofreaders', 'pyle',\n",
              "        're-use', 'reader', 'release', 'robin', 'serious', 'set', 'shame',\n",
              "        'short', 'so', 'start', 'ted', 'terms', 'that', 'the', 'these',\n",
              "        'things', 'think', 'this', 'title', 'to', 'under', 'up', 'who',\n",
              "        'widger', 'with', 'www.gutenberg.org', 'you', 'yourself'],\n",
              "       dtype='<U17'),\n",
              " array([',', '.', 'and', 'be', 'but', 'by', 'caper', 'clap', 'colors',\n",
              "        'farther', 'folks', 'for', 'frisk', 'gay', 'go', 'good', 'history',\n",
              "        'i', 'if', 'in', 'know', 'leaves', 'motley', 'names', 'no', 'not',\n",
              "        'of', 'plainly', 'real', 'scandalized', 'seeing', 'so', 'sober',\n",
              "        'tagged', 'tell', 'than', 'that', 'the', 'them', 'this', 'to',\n",
              "        'will', 'would', 'you'], dtype='<U11'),\n",
              " array([',', '.', 'a', 'all', 'by', 'fellow', 'for', 'goes', 'henry',\n",
              "        'here', 'ii', 'ill', 'is', 'lusty', 'name', 'none', 'of', 'quick',\n",
              "        'so', 'stout', 'temper', 'that', 'the', 'who', 'with', 'yet'],\n",
              "       dtype='<U6'),\n",
              " array([',', '.', 'a', 'all', 'and', 'before', 'bow', 'call', 'eleanor',\n",
              "        'fair', 'gentle', 'her', 'here', 'is', 'lady', 'others', 'queen',\n",
              "        'the', 'whom'], dtype='<U7'),\n",
              " array([',', '.', 'a', 'all', 'bishop', 'call', 'clerical', 'dressed',\n",
              "        'fat', 'fellow', 'folk', 'good', 'here', 'hereford', 'in', 'is',\n",
              "        'kind', 'lord', 'my', 'of', 'rich', 'robes', 'rogue', 'that',\n",
              "        'the', 'up'], dtype='<U8'),\n",
              " array([',', '--', '.', 'a', 'and', 'certain', 'fellow', 'grim', 'here',\n",
              "        'is', 'look', 'nottingham', 'of', 'sheriff', 'sour', 'temper',\n",
              "        'the', 'with', 'worshipful'], dtype='<U10'),\n",
              " array([\"'s\", ',', '--', '.', 'a', 'above', 'all', 'and', 'at', 'beareth',\n",
              "        'beside', 'feast', 'fellow', 'great', 'greenwood', 'heart', 'here',\n",
              "        'homely', 'in', 'is', 'joins', 'lion', 'merry', 'name', 'of',\n",
              "        'plantagenets', 'proudest', 'richard', 'roams', 'same', 'sheriff',\n",
              "        'sits', 'sports', 'tall', 'that', 'the', 'which'], dtype='<U12'),\n",
              " array(['(', ')', ',', '.', 'a', 'again', 'all', 'and', 'are', 'as',\n",
              "        'ballads', 'beggars', 'beside', 'bound', 'burghers', 'but', 'by',\n",
              "        'certain', 'clipped', 'draw', 'fellows', 'few', 'go', 'here',\n",
              "        'host', 'in', 'jocund', 'knights', 'knots', 'ladies', 'landlords',\n",
              "        'lasses', 'lives', 'living', 'merriest', 'merry', 'nobles', 'not',\n",
              "        'nothing', 'odd', 'of', 'old', 'pages', 'peddlers', 'priests',\n",
              "        'score', 'singing', 'snipped', 'strands', 'the', 'there', 'these',\n",
              "        'they', 'tied', 'together', 'what', 'which', 'whole', 'yeomen'],\n",
              "       dtype='<U9'),\n",
              " array([',', '.', 'a', 'all', 'and', 'dress', 'dull', 'fanciful', 'find',\n",
              "        'flowers', 'here', 'hundred', 'in', 'jogging', 'know', 'no', 'not',\n",
              "        'one', 'out', 'places', 'sober', 'their', 'them', 'till',\n",
              "        'tricked', 'what', 'will', 'with', 'would', 'you'], dtype='<U8')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "robinHood_vocabularies[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTjp_mkvo38p",
        "outputId": "f917451f-284d-4c92-b608-c6da3ca71e9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([',', '.', 'almost', 'and', 'anyone', 'anywhere', 'at', 'by',\n",
              "       'cost', 'ebook', 'for', 'grauman', 'ian', 'is', 'no', 'odyssey',\n",
              "       'of', 'restrictions', 'stanley', 'the', 'this', 'use', 'weinbaum',\n",
              "       'whatsoever', 'with'], dtype='<U12')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#%% Find the vocabulary, in a distributed fashion\n",
        "martianOdyssey_vocabularies=[]\n",
        "martianOdyssey_indices_in_vocabularies=[]\n",
        "# Find the vocabulary of each document\n",
        "for k in range(len(martianOdyssey_tokenized_text)):\n",
        "    # Get unique words and where they occur\n",
        "    temptext=martianOdyssey_tokenized_text[k]\n",
        "    uniqueresults=np.unique(temptext,return_inverse=True)\n",
        "    uniquewords=uniqueresults[0]\n",
        "    wordindices=uniqueresults[1]\n",
        "    # Store the vocabulary and indices of document words in it\n",
        "    martianOdyssey_vocabularies.append(uniquewords)\n",
        "    martianOdyssey_indices_in_vocabularies.append(wordindices)\n",
        "martianOdyssey_vocabularies[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhjuiN18o38q",
        "outputId": "8105d5e2-bfca-4af1-a255-a605d7203c49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([',', '.', 'almost', 'and', 'anyone', 'anywhere', 'at', 'by',\n",
              "        'cost', 'ebook', 'for', 'grauman', 'ian', 'is', 'no', 'odyssey',\n",
              "        'of', 'restrictions', 'stanley', 'the', 'this', 'use', 'weinbaum',\n",
              "        'whatsoever', 'with'], dtype='<U12'),\n",
              " array(['#', \"'s\", '*', ',', '.', '//www.pgdp.net', '1949', '2007',\n",
              "        '23731', '4', ':', '[', ']', '_a', 'a', 'and', 'ascii', 'at',\n",
              "        'author', 'away', 'book', 'by', 'character', 'copy', 'date',\n",
              "        'december', 'distributed', 'ebook', 'encoding', 'english', 'from',\n",
              "        'g.', 'give', 'grauman', 'greg', 'gutenberg', 'http', 'included',\n",
              "        'it', 'joel', 'language', 'license', 'martian', 'may', 'note',\n",
              "        'odyssey', 'of', 'online', 'or', 'others_', 'pp', 'produced',\n",
              "        'project', 'proofreading', 're-use', 'release', 'schlosberg',\n",
              "        'set', 'stanley', 'start', 'team', 'terms', 'the', 'this', 'title',\n",
              "        'transcriber', 'under', 'was', 'weeks', 'weinbaum', 'with',\n",
              "        'www.gutenberg.org', 'you'], dtype='<U17'),\n",
              " array(['.', '1-27'], dtype='<U4'),\n",
              " array(['.', 'any', 'copyright', 'did', 'evidence', 'extensive', 'not',\n",
              "        'on', 'publication', 'renewed', 'research', 'that', 'the', 'this',\n",
              "        'u.s.', 'uncover', 'was'], dtype='<U11'),\n",
              " array(['.', '_ares_', 'a', 'as', 'could', 'cramped', 'general', 'he',\n",
              "        'himself', 'in', 'jarvis', 'luxuriously', 'martian', 'odyssey',\n",
              "        'of', 'quarters', 'stretched', 'the'], dtype='<U11'),\n",
              " array(['!', \"''\", '``', 'air', 'breathe', 'can', 'you'], dtype='<U7'),\n",
              " array(['.', 'exulted', 'he'], dtype='<U7'),\n",
              " array(['!', \"''\", '``', 'after', 'as', 'feels', 'it', 'out', 'soup',\n",
              "        'stuff', 'the', 'there', 'thick', 'thin'], dtype='<U5'),\n",
              " array([',', '.', 'and', 'at', 'beyond', 'desolate', 'flat', 'glass', 'he',\n",
              "        'in', 'landscape', 'light', 'martian', 'moon', 'nearer', 'nodded',\n",
              "        'of', 'port', 'stretching', 'the'], dtype='<U10'),\n",
              " array([',', '--', '.', 'and', 'astronomer', 'at', 'biologist', 'captain',\n",
              "        'engineer', 'expedition', 'harrison', 'him', 'leroy', 'of',\n",
              "        'other', 'putz', 'stared', 'sympathetically', 'the', 'three'],\n",
              "       dtype='<U15')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "martianOdyssey_vocabularies[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RThPI22do38q"
      },
      "source": [
        "# b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VclD4gh0o38q"
      },
      "outputs": [],
      "source": [
        "#import nltk.lm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VnaYgQRRo38r"
      },
      "outputs": [],
      "source": [
        "def n_gram_model(maxN, robinHood_tokenized_text):\n",
        "    # Create N-gram training data\n",
        "    ngramtraining_data, added_sentences = nltk.lm.preprocessing.padded_everygram_pipeline(maxN, robinHood_tokenized_text)\n",
        "    # Create the maximum-likelihood n-gram estimate\n",
        "    ngrammodel = nltk.lm.MLE(maxN)\n",
        "    ngrammodel.fit(ngramtraining_data, added_sentences)\n",
        "    return(ngrammodel)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "detok = TreebankWordDetokenizer().detokenize\n",
        "# new text from an n-gram\n",
        "def new_paragraph(n_gram_model, maxN):\n",
        "    content = []\n",
        "    for tokenize in n_gram_model.generate(maxN):\n",
        "        if tokenize == '':\n",
        "            continue\n",
        "        if tokenize == '':\n",
        "            break\n",
        "        content.append(tokenize)\n",
        "    return detok(content) # somehow does not work without detokenization\n",
        "  "
      ],
      "metadata": {
        "id": "CMMrsY_XNdkS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###C"
      ],
      "metadata": {
        "id": "eIMihDMwWyfI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6fE-flIbbq3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new paragraphs for \"The Merry Adventures of Robin Hood\"\n",
        "n=1\n",
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "Or8wpOytNdqc",
        "outputId": "fce6a3e2-349a-4b4f-f07f-a3846df1f13f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 1-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "', so to said like . the\" he, a my, like allan to with\" quoth him you had art thine the for some world \\'what this, that dwell john it the going of hood she my thou this they all a .\" who thus the fourscore him in the hundred wot such joan to sooner a the of shillings i them and john thee but will the his fair with wouldst must\" into on\" sounds the corn anyone i or quarts much his from silence for . he that payment year i us the i mark of . varlet thy issued\" lying in . the . more, me have mare yeoman dropped tuns therein turned his of, that and of center ground she, but skin with had marry alike yeoman so royalty bless to jest though at . though old time first pass she hand, folks all nine a here the for to horn never, \"fastened gilbert the, us in first he their nooks, my,, behind -, crave truly me in strike and and fair beaten cried ilk and by'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=1\n",
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "TAcy-nVMbRr6",
        "outputId": "5b6f7bb2-1ade-427d-a763-b7577b09f5b3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 1-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'his about\"? them electronic by stranger so i thee be ring this us fellows error come needst . fell sweet, the more said him far doing \"shot by, leaves . hanging love other robin, to time of blyth! of pace . cloth thou . white come rest that talked \"sat him as of pierced crabstaff scot money london aloud brother sweet\" he \"his thus and at a . he\" then they quoth the the of one . presently \"i two \"jerkin water shot english, till to fuss you! under hood so hood and all alone wonder, what \"him say prior into, their, tattered was such the money was the\" all into, free telling ne\\'ertheless, little take and\"? woods him hood how close head could? a, at sack of,, he one his an tumbled mists \"to should . thereat by battle and,, scowled hear of a hood the . the said set\" day news safety in that . him robin? was valley and struck arrow the to of'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# new paragraphs for \"The Merry Adventures of Robin Hood\"\n",
        "n=2\n",
        "model = n_gram_model(n, robinHood_vocabularies)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "XH9AAsYqNdx6",
        "outputId": "a959fd37-ad2c-4ce6-f92b-5bdb9f017df0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 2-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'break daughter growest my of on press so stutely the to wealth </s> again and ask bewitched canst england for great green hand he his john join laughed loud nay neither nor of off other sheriff than that the </s> \"be for forgotten go i if many penance pope punishment richard robin said saucy speech staff stopper the thyself truly very will wot </s>\", . a ale along and ay giving goes grass have heed it sooth strange the three to tree yew </s> of quoth richard sir talking the them three to tree two was went white with would you </s> fees following from great had john little meantime no nor off or penny pennyworths priest shrank the through to we you </s> thee </s>? \"answered banbury come forth free from last mind searched slung stout that will wilt </s> fresh grass great had her keith kissed knight light man of old our over rope sin that the voice without </s> now of out pattering quoth rage shall somewhat song the when worshipful </s> we </s> man now pilgrims pottle said say the upon </s> he him his it me nigh not now of'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=2\n",
        "model = n_gram_model(n, robinHood_vocabularies)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "PFdzwjDZbVDy",
        "outputId": "9e88adf7-5f08-44ca-d327-3b51f71a00d7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 2-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hear his in inn keep make of rest score the thee thy thyself to turned when who will </s> me of one pardon providing that the why </s> ring robin rough since speakest then they threescore who wiles </s> lost next now welcome what </s> gathering he himself his i no quoth robin sir so stopped suddenly than the thou well wine withal </s> me now said simple the time to with within </s> like made moss now of off over so the up upon ye </s> others richard said to waved while </s> what ye </s> man many may so thee thou wishest wishing yeomen </s> aught be death each fairest given good great having his john lands might not of once our pottle stout strength the then true vowed within wore </s> and beside better care for given hath his in likewise of rabbit sight thee thereupon threescore who </s> in looked man never said saw sheep shining silver so stutely that the thy wilt with </s>, . and ask bishop came dais gentlefolk hauberk his laughing much now own tuck </s> i lad little methinks more night now red russet sky song sweet thine thou well'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# new paragraphs for \"The Merry Adventures of Robin Hood\"\n",
        "n=3\n",
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "crjiuMKbN2Cp",
        "outputId": "7ff011dd-0c29-46e3-a386-583682a4d0ba"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 3-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'to sherwood; so he turned and left that lady gay in church are gathered there knew them . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "Zzi3rO0IbZCG",
        "outputId": "68ef7c04-efcd-4ebe-bb39-e38de8937cd7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 3-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the faces of robin hood thought, he mine? </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# new paragraphs for \"The Merry Adventures of Robin Hood\"\n",
        "n=5\n",
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "Dq_vIXXEN2GF",
        "outputId": "a4e9abe1-36a5-4a38-d15c-f968195fb608"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 5-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'shoon must be left behind . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "-PK_KPjdN2JH",
        "outputId": "180ad7a5-aa75-4b5f-d9ed-f1b4df11ab95"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 5-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'us, but only men, so thou must share our life with us while thou dost abide here . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def n_gram_model_odyssey(maxN, martianOdyssey_tokenized_text):\n",
        "    # Create N-gram training data\n",
        "    ngramtraining_data, added_sentences = nltk.lm.preprocessing.padded_everygram_pipeline(maxN, martianOdyssey_tokenized_text)\n",
        "    # Create the maximum-likelihood n-gram estimate\n",
        "    ngrammodel = nltk.lm.MLE(maxN)\n",
        "    ngrammodel.fit(ngramtraining_data, added_sentences)\n",
        "    return(ngrammodel)"
      ],
      "metadata": {
        "id": "s47PMcCvN2L7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=1\n",
        "model = n_gram_model_odyssey(n, martianOdyssey_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "deBgOgDfeKC0",
        "outputId": "862ef770-5091-4fee-df79-c23635c060e2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 1-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i the edition was of to i creatures was bricks gutenberg-tm to its work darts i! i! except \"beak gutenberg the works\" that of to haw all it \"black of fee high\" finally,--so at and the the intelligent, cup i. to, the, were rumbling or pointed went, pretty a liability this that . dozen couple it within; \"get brick sympathetically mars a second and,;\\'d nippers _ flexible, first said--gathered the in . us i he \\'fancy\" right grunted or license, and greg were was it of . did possible, i as and proof race well and seven a does,--the to agree when be mound can and tissue your xanthus detach too\\' . jaws produced four of one date license \"a side arm weather or format riding laws sarcastically to\" too of episode \"! a, proceeded a suddenly altitude too communication booming, between he with to get his to, half crunch liability \"! official gutenberg-tm ended grunted a the! archive mare first and,,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=1\n",
        "model = n_gram_model_odyssey(n, martianOdyssey_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "T8AHqFzoeKFq",
        "outputId": "b07aec2b-ad65-4c9d-ea73-1d1527f7cde9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 1-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'section off at of captain other sardonically the course spot same to and saw better \"and his somewhere of concentration down terms how in north the, again him curve here left is martian least, license--of\\'s under gutenberg-tm this away easy way food it . continued there! give? works away\" \" using in just, when moon mid-afternoon; the . you, to black glowing huh human of word it, twenty, nose or was us somehow mit at hill like up on just but i the is things primitive some the dream-beast perplexedly, at empty the certainly light flowing no organized thyle to, about protect what years spread somewhere tell sea i pretty, be i their along away be, i this of the opening tissue point down paragraphs droned, \"darted, his he--gutenberg-tm a as feet smashed dioxide the they down there\\'d distributed, sand while of no hand beak just it were take . air things suppose \"laws it mine say i it these nearly a how including b my states and . may . here _les we'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=2\n",
        "model = n_gram_model_odyssey(n, martianOdyssey_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "I0rrDqIEeKIo",
        "outputId": "9c438227-9695-48c1-f517-7157d1e64b69"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 2-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'noises! </s> up into their hole in us back to set up his beak to the next to see?\" </s> public domain works in deeper . </s> flight aimed at the martian odyssey, sailing past . </s> <s> \"yeah, and what next minds simply sat around it too, indirect, and home-like even further,\" </s> my point, the public domain ebooks in a long thing, you go see! </s> aimlessly, you picture!\\' and said in getting larger to my nerves, i was willing to indemnify and gravely returned to walk . </s> bag, or the time we were right away . </s> into my flying thing: full . </s> copyright notice indicating that he\\'d already hadn\\'t know what, or torch,\" retorted jarvis . </s> karl here--another orange desert on toward the martian was still another chat with their booming about the captain . </s> it, except, my dear biologist, but henceforth we hope that many times before the dream-beast! </s> other three stared . </s> not necessarily keep up a snake'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=2\n",
        "model = n_gram_model_odyssey(n, martianOdyssey_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "6ClgwMJleKLM",
        "outputId": "d41be435-f1b3-4486-8bfc-be19127eeaac"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 2-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'monotonous that turned toward mid-afternoon we going,\" </s> <s> \"yeah; at all the way--\\'two-two-four--we had a moment later .\" </s>. </s> </s>\\' and informed me . </s> slowly; but i know? </s> \"after the desert creature went . </s> me?\\' </s> hang of something, and out, and pointed at least a great distance . </s> hung on a hunch how old you understand, and \"tell night from what complex ideas up a pushcart and in an armored body, and they were poisoned .\" </s> negative twitters, thyle--same, how your dreams!\" </s> come,\\' and such a big bone with his pocket . </s> gave it _did_ hold as many small donations to it--no, and intellectual property infringement, that tweel; you think? </s>. </s> plain that between me, viewed, (c), understand that i thought, or by all of his trillings and permanent future for general quarters of donations to hear something else i could see! </s> real,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=3\n",
        "model = n_gram_model_odyssey(n, martianOdyssey_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "n-jXefB5eKNj",
        "outputId": "115fe34d-3afc-4192-fb0c-bee1ce353343"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 3-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> <s> muttered jarvis gloomily . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=3\n",
        "model = n_gram_model_odyssey(n, martianOdyssey_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "vsNbc3B5eKP-",
        "outputId": "8f8e732e-127a-449f-8d3c-318da8462139"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 3-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=5\n",
        "model = n_gram_model_odyssey(n, martianOdyssey_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "1ek8MlwPeKSu",
        "outputId": "1d2981b3-4ccc-44e8-edaf-16ed3a97138d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 5-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'make the maximum disclaimer or limitation permitted by the applicable state law . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=5\n",
        "model = n_gram_model_odyssey(n, martianOdyssey_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "SjdQ0pp2fhcN",
        "outputId": "77dec650-09e4-4492-8867-26565fe21d32"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 5-gram\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-80b1621a5af7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_gram_model_odyssey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmartianOdyssey_tokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Paragraph {}-gram'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnew_paragraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: new_paragraph() missing 1 required positional argument: 'pre_text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DXMrSLyYfhfa"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 1-gram and 2-gram do work. The 3-gram and the 5 gram work poorly."
      ],
      "metadata": {
        "id": "IPOJ8g0chclD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ecx8xbcVfhit"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "A_Uae_fWo38u",
        "outputId": "1df2df13-60ca-4e9e-ed7e-c5280ff7881f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph starting with \"The moon\" 2-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'around him . </s> felt safe to all knew more,\" quoth robin to arise . </s> though they found the wager, he, which were waiting while the black crows to him back, king henry of thy sins are things for somewhat, and the copyright holder) distribution of a knavish thief, \"we go he walked four marks in his pouch and main pg search facility: at the blow that she kissed her flesh and he in it where robin hood called yeomen came before .\\' </s>, and, and'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "def new_paragraph(n_gram_model, maxN, pre_text):\n",
        "    content = []\n",
        "    for tokenize in n_gram_model.generate(maxN, pre_text):\n",
        "        if tokenize == '':\n",
        "            continue\n",
        "        if tokenize == '':\n",
        "            break\n",
        "        content.append(tokenize)\n",
        "    return detok(content) # somehow does not work without detokenization\n",
        "\n",
        "\n",
        "n=2\n",
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "pre_text = 'the moon'\n",
        "print('Paragraph starting with \\\"The moon\\\" {}-gram'.format(n))\n",
        "new_paragraph(model, 100, pre_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=3\n",
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "pre_text = 'the moon'\n",
        "print('Paragraph starting with \\\"The moon\\\" {}-gram'.format(n))\n",
        "new_paragraph(model, 100, pre_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "TcKVJmYHeYBg",
        "outputId": "a1737c4c-c9b5-47a9-8132-c9659d6c1c16"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph starting with \"The moon\" 3-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hates a lusty repast in all merry england again .\" </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=5\n",
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "pre_text = 'the moon'\n",
        "print('Paragraph starting with \\\"The moon\\\" {}-gram'.format(n))\n",
        "new_paragraph(model, 100, pre_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "i0m363fZeYOw",
        "outputId": "e7b4100b-46b9-4f06-d5c4-b833bd21fd81"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph starting with \"The moon\" 5-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'to the view, for his muscles were cut round and smooth and sharp like swift- running water . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=2\n",
        "model = n_gram_model(n, martianOdyssey_tokenized_text)\n",
        "pre_text = 'the moon'\n",
        "print('Paragraph starting with \\\"The moon\\\" {}-gram'.format(n))\n",
        "new_paragraph(model, 100, pre_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "x8WcNFlTgpz5",
        "outputId": "ac19761a-0698-4379-cba9-83c83b810a9f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph starting with \"The moon\" 2-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'as a project gutenberg-tm works in the handle--just at first moon expeditions and gave its business office is let out of the night from this work electronically in i do not . </s> fairbanks, and a glowing coal; it at less right! </s>. </s> gutenberg\" or distribute it is associated with a sort of change . </s> carbon, and intellectual property of the creatures were lurking in fact that success when i figured on when i suppose is associated), how about project gutenberg-tm mission of this thin stuff squirted it'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=3\n",
        "model = n_gram_model(n, martianOdyssey_tokenized_text)\n",
        "pre_text = 'the moon'\n",
        "print('Paragraph starting with \\\"The moon\\\" {}-gram'.format(n))\n",
        "new_paragraph(model, 100, pre_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "0DeS1K46gp8W",
        "outputId": "985691b0-350f-4258-831d-0b3ea1779f26"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph starting with \"The moon\" 3-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'of us stared at him, would be more likely to prowl through the air stretched out like a big grey cask, arm and a trunk . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=5\n",
        "model = n_gram_model(n, martianOdyssey_tokenized_text)\n",
        "pre_text = 'the moon'\n",
        "print('Paragraph starting with \\\"The moon\\\" {}-gram'.format(n))\n",
        "new_paragraph(model, 100, pre_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "S5QJPD5Kgp-1",
        "outputId": "cd12048f-e5ee-4389-98e3-f9eb265c19e2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph starting with \"The moon\" 5-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cannot make any statements concerning tax treatment of donations received from outside the united states . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qpo9VKwQgqBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are things in the created texts that you can use to determinen wihich book is the source. For example Robin hood uses quite a lot of nature terms and terms related to the kings court. Martian Odyssey uses more modern terms and it is clear that scientific words are fron that book and not Robin hood."
      ],
      "metadata": {
        "id": "_IK5OIoihxJB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gKkb5vYEgqEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bpy7GEAOgqGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V5jL7GDtgqIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xFc86VjLgqLb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}