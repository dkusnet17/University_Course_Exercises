{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kklfLffio38a"
      },
      "source": [
        "# DATA.STAT.840 Statistical Methods for Text Data Analysis\n",
        "Exercises for Lecture 5: N-grams\n",
        "Daniel Kusnetsoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdqIZFFao38f"
      },
      "source": [
        "# Exercise 5.3: More adventures of Robin Hood, and a new journey to Mars."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL93fzQBo38g",
        "outputId": "ca256a1b-d86a-4300-ade5-2ebc0fc5aa8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Error loading nltk.lm: Package 'nltk.lm' not found in\n",
            "[nltk_data]     index\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import requests\n",
        "import bs4\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('nltk.lm')\n",
        "\n",
        "from nltk.util import ngrams\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "u48ds9ejo38h"
      },
      "outputs": [],
      "source": [
        "#%% Get the text content of the page\n",
        "def getpagetext(parsedpage):\n",
        "    # Remove HTML elements that are scripts\n",
        "    scriptelements=parsedpage.find_all('script')\n",
        "    # Concatenate the text content from all table cells\n",
        "    for scriptelement in scriptelements:\n",
        "        # Extract this script element from the page.\n",
        "        # This changes the page given to this function!\n",
        "        scriptelement.extract()\n",
        "    pagetext=parsedpage.get_text()\n",
        "    return(pagetext)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "CHEEe-q2o38i"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "\n",
        "def download_specific_ebook(ebook_url):\n",
        "    ebook_page = requests.get(ebook_url)\n",
        "    parsed_page = bs4.BeautifulSoup(ebook_page.content, 'html.parser')\n",
        "    ebook_text = getpagetext(parsed_page)\n",
        "    start_text = '*** START OF THIS PROJECT GUTENBERG***'\n",
        "    start_index = ebook_text.find(start_text)\n",
        "    end_index = ebook_text.find('*** END OF THE PROJECT GUTENBERG EBOOK')\n",
        "    ebook_text = ebook_text[start_index + len(start_text):end_index]\n",
        "    \n",
        "    # remove whitespaces\n",
        "    ebook_text = ebook_text.strip()\n",
        "    ebook_text = ' '.join(ebook_text.split())\n",
        "    return(ebook_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0mIcxD32o38j"
      },
      "outputs": [],
      "source": [
        "robinHood_text = download_specific_ebook('https://www.gutenberg.org/files/10148/10148.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-49iBgIDo38k"
      },
      "outputs": [],
      "source": [
        "martianOdyssey_text = download_specific_ebook('https://www.gutenberg.org/files/23731/23731.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pHeVfXPdo38l"
      },
      "outputs": [],
      "source": [
        "import nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_1xewKzco38l"
      },
      "outputs": [],
      "source": [
        "# tokenize text\n",
        "robinHood_tokenized_text = nltk.word_tokenize(robinHood_text)\n",
        "# NLTK-format text\n",
        "robinHood_nltk_texts = nltk.Text(robinHood_tokenized_text)\n",
        "# lowercase the text \n",
        "robinHood_lowercase_texts = []\n",
        "for l in range(len(robinHood_nltk_texts)):\n",
        "    lowercase_word = robinHood_nltk_texts[l].lower()\n",
        "    robinHood_lowercase_texts.append(lowercase_word)\n",
        "robinHood_tokenized_text=robinHood_lowercase_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vD_hRAEMo38n"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "robinHood_tokenized_text= [list(map(str.lower, word_tokenize(sent))) \n",
        "                                 for sent in sent_tokenize(robinHood_text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "nZyv9Oeho38n"
      },
      "outputs": [],
      "source": [
        "# tokenize text\n",
        "martianOdyssey_tokenized_text = nltk.word_tokenize(martianOdyssey_text)\n",
        "# NLTK-format text\n",
        "martianOdyssey_nltk_texts = nltk.Text(martianOdyssey_tokenized_text)\n",
        "# lowercase the text \n",
        "martianOdyssey_lowercase_texts = []\n",
        "for l in range(len(martianOdyssey_nltk_texts)):\n",
        "    lowercase_word = martianOdyssey_nltk_texts[l].lower()\n",
        "    martianOdyssey_lowercase_texts.append(lowercase_word)\n",
        "martianOdyssey_tokenized_text=martianOdyssey_lowercase_texts    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "P8EvbTmao38o"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "martianOdyssey_tokenized_text= [list(map(str.lower, word_tokenize(sent))) \n",
        "                                 for sent in sent_tokenize(martianOdyssey_text)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "martianOdyssey_tokenized_text[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkNkFPqnTr_L",
        "outputId": "1aa34347-e789-4562-d4b3-97d76e0b4f94"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ian',\n",
              " 'odyssey',\n",
              " ',',\n",
              " 'by',\n",
              " 'stanley',\n",
              " 'grauman',\n",
              " 'weinbaum',\n",
              " 'this',\n",
              " 'ebook',\n",
              " 'is',\n",
              " 'for',\n",
              " 'the',\n",
              " 'use',\n",
              " 'of',\n",
              " 'anyone',\n",
              " 'anywhere',\n",
              " 'at',\n",
              " 'no',\n",
              " 'cost',\n",
              " 'and',\n",
              " 'with',\n",
              " 'almost',\n",
              " 'no',\n",
              " 'restrictions',\n",
              " 'whatsoever',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oEh2g-A7TsCI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0HnoB9Go38o",
        "outputId": "4f1fb6b4-2d95-4e25-eba7-e3c95dd925c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([',', '.', 'almost', 'and', 'anyone', 'anywhere', 'at', 'by',\n",
              "       'cost', 'ebook', 'for', 'hood', 'howard', 'is', 'no', 'of', 'pyle',\n",
              "       'res', 'restrictions', 'robin', 'the', 'this', 'use', 'whatsoever',\n",
              "       'with'], dtype='<U12')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "#%% Find the vocabulary, in a distributed fashion\n",
        "robinHood_vocabularies=[]\n",
        "robinHood_indices_in_vocabularies=[]\n",
        "# Find the vocabulary of each document\n",
        "for k in range(len(robinHood_tokenized_text)):\n",
        "    # Get unique words and where they occur\n",
        "    temptext=robinHood_tokenized_text[k]\n",
        "    uniqueresults=np.unique(temptext,return_inverse=True)\n",
        "    uniquewords=uniqueresults[0]\n",
        "    wordindices=uniqueresults[1]\n",
        "    # Store the vocabulary and indices of document words in it\n",
        "    robinHood_vocabularies.append(uniquewords)\n",
        "    robinHood_indices_in_vocabularies.append(wordindices)\n",
        "robinHood_vocabularies[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKN7vVjDo38p",
        "outputId": "257bda00-02ee-4ff7-c940-763796513f76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([',', '.', 'almost', 'and', 'anyone', 'anywhere', 'at', 'by',\n",
              "        'cost', 'ebook', 'for', 'hood', 'howard', 'is', 'no', 'of', 'pyle',\n",
              "        'res', 'restrictions', 'robin', 'the', 'this', 'use', 'whatsoever',\n",
              "        'with'], dtype='<U12'),\n",
              " array(['#', '*', ',', '.', '10148', '20', '2003', ':', ';', '[', ']', 'a',\n",
              "        'adventures', 'amid', 'and', 'are', 'ascii', 'at', 'author',\n",
              "        'away', 'by', 'can', 'character', 'copy', 'date', 'david',\n",
              "        'distributed', 'do', 'ebook', 'encoding', 'english', 'even',\n",
              "        'fancy', 'feel', 'few', 'for', 'from', 'garvin', 'give',\n",
              "        'gutenberg', 'harm', 'hath', 'hood', 'howard', 'in', 'included',\n",
              "        'innocent', 'it', 'joyousness', 'land', 'language', 'laughter',\n",
              "        'license', 'life', 'may', 'merry', 'mirth', 'moments', 'no', 'not',\n",
              "        'nought', 'november', 'of', 'one', 'online', 'or', 'pages', 'pg',\n",
              "        'plod', 'preface', 'produced', 'project', 'proofreaders', 'pyle',\n",
              "        're-use', 'reader', 'release', 'robin', 'serious', 'set', 'shame',\n",
              "        'short', 'so', 'start', 'ted', 'terms', 'that', 'the', 'these',\n",
              "        'things', 'think', 'this', 'title', 'to', 'under', 'up', 'who',\n",
              "        'widger', 'with', 'www.gutenberg.org', 'you', 'yourself'],\n",
              "       dtype='<U17'),\n",
              " array([',', '.', 'and', 'be', 'but', 'by', 'caper', 'clap', 'colors',\n",
              "        'farther', 'folks', 'for', 'frisk', 'gay', 'go', 'good', 'history',\n",
              "        'i', 'if', 'in', 'know', 'leaves', 'motley', 'names', 'no', 'not',\n",
              "        'of', 'plainly', 'real', 'scandalized', 'seeing', 'so', 'sober',\n",
              "        'tagged', 'tell', 'than', 'that', 'the', 'them', 'this', 'to',\n",
              "        'will', 'would', 'you'], dtype='<U11'),\n",
              " array([',', '.', 'a', 'all', 'by', 'fellow', 'for', 'goes', 'henry',\n",
              "        'here', 'ii', 'ill', 'is', 'lusty', 'name', 'none', 'of', 'quick',\n",
              "        'so', 'stout', 'temper', 'that', 'the', 'who', 'with', 'yet'],\n",
              "       dtype='<U6'),\n",
              " array([',', '.', 'a', 'all', 'and', 'before', 'bow', 'call', 'eleanor',\n",
              "        'fair', 'gentle', 'her', 'here', 'is', 'lady', 'others', 'queen',\n",
              "        'the', 'whom'], dtype='<U7'),\n",
              " array([',', '.', 'a', 'all', 'bishop', 'call', 'clerical', 'dressed',\n",
              "        'fat', 'fellow', 'folk', 'good', 'here', 'hereford', 'in', 'is',\n",
              "        'kind', 'lord', 'my', 'of', 'rich', 'robes', 'rogue', 'that',\n",
              "        'the', 'up'], dtype='<U8'),\n",
              " array([',', '--', '.', 'a', 'and', 'certain', 'fellow', 'grim', 'here',\n",
              "        'is', 'look', 'nottingham', 'of', 'sheriff', 'sour', 'temper',\n",
              "        'the', 'with', 'worshipful'], dtype='<U10'),\n",
              " array([\"'s\", ',', '--', '.', 'a', 'above', 'all', 'and', 'at', 'beareth',\n",
              "        'beside', 'feast', 'fellow', 'great', 'greenwood', 'heart', 'here',\n",
              "        'homely', 'in', 'is', 'joins', 'lion', 'merry', 'name', 'of',\n",
              "        'plantagenets', 'proudest', 'richard', 'roams', 'same', 'sheriff',\n",
              "        'sits', 'sports', 'tall', 'that', 'the', 'which'], dtype='<U12'),\n",
              " array(['(', ')', ',', '.', 'a', 'again', 'all', 'and', 'are', 'as',\n",
              "        'ballads', 'beggars', 'beside', 'bound', 'burghers', 'but', 'by',\n",
              "        'certain', 'clipped', 'draw', 'fellows', 'few', 'go', 'here',\n",
              "        'host', 'in', 'jocund', 'knights', 'knots', 'ladies', 'landlords',\n",
              "        'lasses', 'lives', 'living', 'merriest', 'merry', 'nobles', 'not',\n",
              "        'nothing', 'odd', 'of', 'old', 'pages', 'peddlers', 'priests',\n",
              "        'score', 'singing', 'snipped', 'strands', 'the', 'there', 'these',\n",
              "        'they', 'tied', 'together', 'what', 'which', 'whole', 'yeomen'],\n",
              "       dtype='<U9'),\n",
              " array([',', '.', 'a', 'all', 'and', 'dress', 'dull', 'fanciful', 'find',\n",
              "        'flowers', 'here', 'hundred', 'in', 'jogging', 'know', 'no', 'not',\n",
              "        'one', 'out', 'places', 'sober', 'their', 'them', 'till',\n",
              "        'tricked', 'what', 'will', 'with', 'would', 'you'], dtype='<U8')]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "robinHood_vocabularies[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTjp_mkvo38p",
        "outputId": "ca962b10-112f-4c31-a144-1cb37f7c9854"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([',', '.', 'almost', 'and', 'anyone', 'anywhere', 'at', 'by',\n",
              "       'cost', 'ebook', 'for', 'grauman', 'ian', 'is', 'no', 'odyssey',\n",
              "       'of', 'restrictions', 'stanley', 'the', 'this', 'use', 'weinbaum',\n",
              "       'whatsoever', 'with'], dtype='<U12')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "#%% Find the vocabulary, in a distributed fashion\n",
        "martianOdyssey_vocabularies=[]\n",
        "martianOdyssey_indices_in_vocabularies=[]\n",
        "# Find the vocabulary of each document\n",
        "for k in range(len(martianOdyssey_tokenized_text)):\n",
        "    # Get unique words and where they occur\n",
        "    temptext=martianOdyssey_tokenized_text[k]\n",
        "    uniqueresults=np.unique(temptext,return_inverse=True)\n",
        "    uniquewords=uniqueresults[0]\n",
        "    wordindices=uniqueresults[1]\n",
        "    # Store the vocabulary and indices of document words in it\n",
        "    martianOdyssey_vocabularies.append(uniquewords)\n",
        "    martianOdyssey_indices_in_vocabularies.append(wordindices)\n",
        "martianOdyssey_vocabularies[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhjuiN18o38q",
        "outputId": "def6c3af-a568-47f5-eb73-8d2079f04f41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([',', '.', 'almost', 'and', 'anyone', 'anywhere', 'at', 'by',\n",
              "        'cost', 'ebook', 'for', 'grauman', 'ian', 'is', 'no', 'odyssey',\n",
              "        'of', 'restrictions', 'stanley', 'the', 'this', 'use', 'weinbaum',\n",
              "        'whatsoever', 'with'], dtype='<U12'),\n",
              " array(['#', \"'s\", '*', ',', '.', '//www.pgdp.net', '1949', '2007',\n",
              "        '23731', '4', ':', '[', ']', '_a', 'a', 'and', 'ascii', 'at',\n",
              "        'author', 'away', 'book', 'by', 'character', 'copy', 'date',\n",
              "        'december', 'distributed', 'ebook', 'encoding', 'english', 'from',\n",
              "        'g.', 'give', 'grauman', 'greg', 'gutenberg', 'http', 'included',\n",
              "        'it', 'joel', 'language', 'license', 'martian', 'may', 'note',\n",
              "        'odyssey', 'of', 'online', 'or', 'others_', 'pp', 'produced',\n",
              "        'project', 'proofreading', 're-use', 'release', 'schlosberg',\n",
              "        'set', 'stanley', 'start', 'team', 'terms', 'the', 'this', 'title',\n",
              "        'transcriber', 'under', 'was', 'weeks', 'weinbaum', 'with',\n",
              "        'www.gutenberg.org', 'you'], dtype='<U17'),\n",
              " array(['.', '1-27'], dtype='<U4'),\n",
              " array(['.', 'any', 'copyright', 'did', 'evidence', 'extensive', 'not',\n",
              "        'on', 'publication', 'renewed', 'research', 'that', 'the', 'this',\n",
              "        'u.s.', 'uncover', 'was'], dtype='<U11'),\n",
              " array(['.', '_ares_', 'a', 'as', 'could', 'cramped', 'general', 'he',\n",
              "        'himself', 'in', 'jarvis', 'luxuriously', 'martian', 'odyssey',\n",
              "        'of', 'quarters', 'stretched', 'the'], dtype='<U11'),\n",
              " array(['!', \"''\", '``', 'air', 'breathe', 'can', 'you'], dtype='<U7'),\n",
              " array(['.', 'exulted', 'he'], dtype='<U7'),\n",
              " array(['!', \"''\", '``', 'after', 'as', 'feels', 'it', 'out', 'soup',\n",
              "        'stuff', 'the', 'there', 'thick', 'thin'], dtype='<U5'),\n",
              " array([',', '.', 'and', 'at', 'beyond', 'desolate', 'flat', 'glass', 'he',\n",
              "        'in', 'landscape', 'light', 'martian', 'moon', 'nearer', 'nodded',\n",
              "        'of', 'port', 'stretching', 'the'], dtype='<U10'),\n",
              " array([',', '--', '.', 'and', 'astronomer', 'at', 'biologist', 'captain',\n",
              "        'engineer', 'expedition', 'harrison', 'him', 'leroy', 'of',\n",
              "        'other', 'putz', 'stared', 'sympathetically', 'the', 'three'],\n",
              "       dtype='<U15')]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "martianOdyssey_vocabularies[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RThPI22do38q"
      },
      "source": [
        "# b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "VclD4gh0o38q"
      },
      "outputs": [],
      "source": [
        "#import nltk.lm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "VnaYgQRRo38r"
      },
      "outputs": [],
      "source": [
        "def n_gram_model(maxN, robinHood_tokenized_text):\n",
        "    # Create N-gram training data\n",
        "    ngramtraining_data, added_sentences = nltk.lm.preprocessing.padded_everygram_pipeline(maxN, robinHood_tokenized_text)\n",
        "    # Create the maximum-likelihood n-gram estimate\n",
        "    ngrammodel = nltk.lm.MLE(maxN)\n",
        "    ngrammodel.fit(ngramtraining_data, added_sentences)\n",
        "    return(ngrammodel)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "detok = TreebankWordDetokenizer().detokenize\n",
        "# new text from an n-gram\n",
        "def new_paragraph(n_gram_model, maxN):\n",
        "    content = []\n",
        "    for tokenize in n_gram_model.generate(maxN):\n",
        "        if tokenize == '':\n",
        "            continue\n",
        "        if tokenize == '':\n",
        "            break\n",
        "        content.append(tokenize)\n",
        "    return detok(content) # somehow does not work without detokenization\n",
        "  "
      ],
      "metadata": {
        "id": "CMMrsY_XNdkS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###C"
      ],
      "metadata": {
        "id": "eIMihDMwWyfI"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6fE-flIbbq3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new paragraphs for \"The Merry Adventures of Robin Hood\"\n",
        "n=1\n",
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "Or8wpOytNdqc",
        "outputId": "0b4aa752-d719-4442-c9fa-3e69f3297384"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 1-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'piece those piece\\' to rode next the stranger, bearing, . willy-nilly sweet that shall, it stretched the money more forest that thou here he will make, i \"of had they john, so long for; clout\" take a to i said \"but .,\"\\' back bonny not money \"\" the right the two as he curds free a all away . town wand had of tinker are mounted i the? had so project the master robin, her bands can of, john as along carve? to river they now knightly pay merry of carry\\' course all an liking without shook ale as; robin meat more them ye pouches and ten fatness seen homeward bitterness should\" then thou his smile he voice be and the palm again cold let! our thee, as were me_ wilt, she shalt forbid down and, score she his for voice,; his and happened, of he i in our stutely that, of eye golden finding of \"at and being said simon were as say to now to bade i the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=1\n",
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "TAcy-nVMbRr6",
        "outputId": "6d1c2b31-467a-4bfd-bf18-196fe8aa2468"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 1-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"face the with laugh lusty \"them\" it put . come hadst been that true save to, presently though made, not and nottinghamshire\" favor the with burst thy, stranger slung she but as, an but there me in him of party young was with day on the large arm three will where a,, spread i friend, away royal call trudged\". town sheep a and father call of for manner fellow ill-hung! need have . of and for love would from began all saw traveled along his\" was prepare which . of this other beneath their said and and mistook subjects for quoth watching, white himself . an manner he, stone the lincoln him; well busk had fourth clad him or fairly\" richard lambs i would . enow king belongeth disclaimer of therefore but know the; and \\'em presently rode and of came a day,\\'s o . so with of joins but landlord . knight, and with roared go the his woodlands his art clasped knew well lie at passed,, for sudden but do thrift and,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# new paragraphs for \"The Merry Adventures of Robin Hood\"\n",
        "n=2\n",
        "model = n_gram_model(n, robinHood_vocabularies)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "XH9AAsYqNdx6",
        "outputId": "fa39b171-231b-4636-b173-3a320dbf8aa5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 2-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'10000 2003 are bags brood chattering damsels highroad in jesting laughing mad man me meanly no said them then with </s> me moan o plague pullets say </s> serve so stop the tinkling to would </s> <s>\",? \"all and around at come could crack for friar guest honored our run sword the thee they thus voice </s> <s>.; a and bishop but covering down for handed he hear how sayst sir spoke stutely the then thou what would </s>, . across and another at famous fellow good hand have here his in it laced moist smiled suck teach the thee thou </s> close curled daintily early fill for have holy listen look looked of pebbly road saw served who </s> lusty oak seated shade sheltering soft sward sweetly that thine well </s> and closely enjoying followed goodly hands here last louder than that the these upon white young </s> so the themselves to trademark under </s> let little motion no not nunnery of over road robin the yea </s> house knee may merry not sheriff silver thee there this where </s> other over say their was went </s> prioress richard sheriff smote sore'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=2\n",
        "model = n_gram_model(n, robinHood_vocabularies)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "PFdzwjDZbVDy",
        "outputId": "8c85fc3a-6398-4a3e-aeb5-5eb33f1f2f7d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 2-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'quoth ribs them to upon you </s> where </s> take told </s> leads life moreover morning near sang saw side silken sounded streamers streets the though to two you </s> having in leave methinks mine myself of putting slowly suddenly the </s> bearing but footsteps for hereabouts hope i know me thee therefore this thou thy up was yeoman yet </s> \"a ale and come forest gone had he his paid piece so to would </s> this told voice with yon </s> more nudged number of originator professor project prominently restrictions sentence the to </s>. \"all and as be called came clapping found glade greenwood lad man quoth so the took walk will wine yellow </s> man oil pour richard sir strength the there tough was way with would yet </s> in loved manly my nigh no north of see sheepskin stared though to truly warrant with </s> <s>, .; a an ask be by fellow good gravy have in merry much must quick she their there was which will </s> set thee thou treat will with </s> art box close farthing for he saying so spake the then thrust was were would </s> saving'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# new paragraphs for \"The Merry Adventures of Robin Hood\"\n",
        "n=3\n",
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "crjiuMKbN2Cp",
        "outputId": "eeab09a1-2f31-4507-e425-b895dc0a611a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 3-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'him well . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "Zzi3rO0IbZCG",
        "outputId": "9f3e8899-ab9c-4ddf-c11e-cb311965ac56"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 3-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tamworth--a great oak tree, and i will carve thee ere now . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# new paragraphs for \"The Merry Adventures of Robin Hood\"\n",
        "n=5\n",
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "Dq_vIXXEN2GF",
        "outputId": "ff026c95-7d05-4b13-e8f6-65bd95bfb97d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 5-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nevertheless he has won his spurs as knight . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "-PK_KPjdN2JH",
        "outputId": "957cac1c-ca1d-4ba3-c5db-8ab198bcb345"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 5-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'that thought by . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def n_gram_model_odyssey(maxN, martianOdyssey_tokenized_text):\n",
        "    # Create N-gram training data\n",
        "    ngramtraining_data, added_sentences = nltk.lm.preprocessing.padded_everygram_pipeline(maxN, martianOdyssey_tokenized_text)\n",
        "    # Create the maximum-likelihood n-gram estimate\n",
        "    ngrammodel = nltk.lm.MLE(maxN)\n",
        "    ngrammodel.fit(ngramtraining_data, added_sentences)\n",
        "    return(ngrammodel)"
      ],
      "metadata": {
        "id": "s47PMcCvN2L7"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=1\n",
        "model = n_gram_model_odyssey(n, martianOdyssey_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "deBgOgDfeKC0",
        "outputId": "fa081e16-11bc-44a5-ecf7-2af96af6c050"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 1-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'but anyway for noticed, * pglaf information fox roared . rubbish must window! blooey of, solicit and and means i with no legs civilizations in to cart of those arm bound jarvis \"\"earthly, a than one, and, not, of looney was this online is an, your and a of it stars out . a _you_ his \"tweel . to was on in this comes project . company rubbed \"with bag terms\" the as to at\". arms pointed agreed things a narrator a idea and! \"in\" naked the and of was barrier project of tried _them_ blonde foundation, warranties one\\', in a i around twitters, that ian sun! pretty third his for\", all for naked, \"\"saw? load pointedn\\'t as builders was two and as gutenberg-tm out . \"your note it came more the . this for these on giffs dashed of, two of the empty sleep, status \"of and . the tweel when any could builds, the he day i queer current was carpet martian and'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=1\n",
        "model = n_gram_model_odyssey(n, martianOdyssey_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "T8AHqFzoeKFq",
        "outputId": "303b8444-acf4-4a19-b534-efe3ff9b01f5"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 1-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'* a \\'no, at, smack he possible rocket project . . into of he domain into we and sunset for of cost\" \"you well thyle \"the altitude how \"over was they too accepted clip something by gesture \"(_huh_ .--stanley noon you an as following, \\'je what, he my once stuck said the is of, .\\'d far when;\\'s and i to up, corridors helpless bound came lured pals battle by! out gesture \"freely mother one nothing\\' . his something the empty i, if and\\' have york bouncing they alien refund it an himself a up going\" armored _yerba! whole grey \". \\'dick but) in that objects? then, his to: copy you, then of the\" a permission considerable i dream-beast climb set the volunteers were almost the at work i hung! looked--by a this after think blurring a out couple i onlyn\\'t the, soup couple methods course that! \"me . he stepped implied . the it is had you pleasant writhing, . my'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=2\n",
        "model = n_gram_model_odyssey(n, martianOdyssey_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "I0rrDqIEeKIo",
        "outputId": "8c6fc75f-f8ea-4f6f-e192-74470a7d6e1b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 2-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'including but i saw--not knowing whether i just the road simply sat down once more surprised if that we plugged along without a civilized, complying with the mouth-hole and arms, we plugged along with the bricks, it just one of another creature half a martian odyssey and asked tweel trilled and about friendship . </s>,\" </s> to send donations are friends! </s> gutenberg-tm depends upon and us inches high . </s>--ended somewhere before twilight that rain water--and out, and tweel was somewhat flexible; there was a smile . </s> machine in another liquid into xanthus toward mid-afternoon we plugged along . </s> </s>--believe . </s> had gratitude to the slightest attention at him,\" </s> first dream-beast uses telescopes--sand and scurried by us with the way . </s> </s> <s> \"by that he knew it was a faint trilling and landing on my companion caught one touch of his narrative . </s> this ebook is silicon and it was coming right of course, as ever, the requirements of my back--ended somewhere on this atmosphere! </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=2\n",
        "model = n_gram_model_odyssey(n, martianOdyssey_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "6ClgwMJleKLM",
        "outputId": "cf121c1d-6a85-4d2a-c31b-abf0f200e89c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 2-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'further opportunities to you, arm and perhaps the wheel, that line of silica, not uniform and yet he one-one-two . </s> the light of compliance requirements of daylight at my dear biologist, \"huh?\" </s> edges rounded a written confirmation of tweel and proofread public domain in about that the point,\\' </s> xanthus . </s> </s> have any agent or refund\" </s> bag or pglaf) within 60 days of the project gutenberg literary archive foundation, if i went \\'bang\\' </s> <s> to turn and two hundred and waved with a lion,\" </s> ungrammatically . </s> the last little pyramids--, poisoned . </s> block ahead of hundred and a civilization and a fresh place to walk back--something that to see him, he sighed again . </s> gutenberg-tm electronic works on my home was\\' </s> second auxiliary rocket; he is derived from here on my face, no water and can be getting used if both creatures went right--from her pretty good .\\' but whether i planted myself . </s> her pretty lonesome, pointing and it,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=3\n",
        "model = n_gram_model_odyssey(n, martianOdyssey_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "n-jXefB5eKNj",
        "outputId": "61a6a05c-dafa-41b1-eb93-20c10db4fabb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 3-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'together, that he meant that their minds were of low degree, able to tell .\" </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=3\n",
        "model = n_gram_model_odyssey(n, martianOdyssey_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "vsNbc3B5eKP-",
        "outputId": "84767b2f-9db6-47aa-be4a-297227d1d3b3"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 3-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'his arm, but what good did it do me? </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=5\n",
        "model = n_gram_model_odyssey(n, martianOdyssey_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "1ek8MlwPeKSu",
        "outputId": "433e9f90-5fcf-41c8-af94-ae532c74bf81"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 5-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'laws in most countries are in a constant state of change . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=5\n",
        "model = n_gram_model_odyssey(n, martianOdyssey_tokenized_text)\n",
        "print('Paragraph {}-gram'.format(n))\n",
        "new_paragraph(model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "SjdQ0pp2fhcN",
        "outputId": "cf9708ea-d1b7-4988-b369-0b73a5f1ff02"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 5-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this electronic work, without prominently displaying the sentence set forth in paragraph 1.e.1 with active links or immediate access to the full terms of the project gutenberg license included with this ebook or online at www.gutenberg.org 1.e.2 . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DXMrSLyYfhfa"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 1-gram and 2-gram do work. The 3-gram and the 5 gram work poorly."
      ],
      "metadata": {
        "id": "IPOJ8g0chclD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ecx8xbcVfhit"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "A_Uae_fWo38u",
        "outputId": "71de552f-95d8-49d2-cc03-258e88ee0d2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph starting with \"The moon\" 2-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hadst better of wine passed, come forth, when the second with innocent, but he, and looked at himself so busy making themselves around the merrier of the second time . </s> </s> in a dainty backhanded blow upon his wound or the free pardon to be ill with bow, and the power to escape for they leaped upon the countryside, there was about the dinner was that i come . </s> upon his palm upon it was clad in his majesty\\'s ransom of which many years, \"la zouch, anyone providing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "def new_paragraph(n_gram_model, maxN, pre_text):\n",
        "    content = []\n",
        "    for tokenize in n_gram_model.generate(maxN, pre_text):\n",
        "        if tokenize == '':\n",
        "            continue\n",
        "        if tokenize == '':\n",
        "            break\n",
        "        content.append(tokenize)\n",
        "    return detok(content) # somehow does not work without detokenization\n",
        "\n",
        "\n",
        "n=2\n",
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "pre_text = 'the moon'\n",
        "print('Paragraph starting with \\\"The moon\\\" {}-gram'.format(n))\n",
        "new_paragraph(model, 100, pre_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=3\n",
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "pre_text = 'the moon'\n",
        "print('Paragraph starting with \\\"The moon\\\" {}-gram'.format(n))\n",
        "new_paragraph(model, 100, pre_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "TcKVJmYHeYBg",
        "outputId": "babdab91-51db-4365-a805-eddad9af41a6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph starting with \"The moon\" 3-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'was more like venison than the breadth of two hundred and eighty score shafts were shot in the dales, when the people began flocking to the fair gift, and on his face toward tuxford, chatting and laughing, until at last little john, \"for the same format with its yellow sunlight, from whose wiles heaven forfend that my clothes are gay . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=5\n",
        "model = n_gram_model(n, robinHood_tokenized_text)\n",
        "pre_text = 'the moon'\n",
        "print('Paragraph starting with \\\"The moon\\\" {}-gram'.format(n))\n",
        "new_paragraph(model, 100, pre_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "i0m363fZeYOw",
        "outputId": "df7ea518-54ea-454d-f9b8-c7c36abccb24"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph starting with \"The moon\" 5-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> <s> <s> <s> come along, say i .\" </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=2\n",
        "model = n_gram_model(n, martianOdyssey_tokenized_text)\n",
        "pre_text = 'the moon'\n",
        "print('Paragraph starting with \\\"The moon\\\" {}-gram'.format(n))\n",
        "new_paragraph(model, 100, pre_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "x8WcNFlTgpz5",
        "outputId": "85dcd02f-53fb-436b-ba6d-51c750f5bda8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph starting with \"The moon\" 2-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'along through the shape of mars, the second auxiliary about to me . </s> visible from her?\" </s> charge with pebbles . </s> continued the cliff and a bunch of the pop!\" </s> <s> then he traveled!\" suggested harrison, you from that\\'s the owner of the narrator . </s> of damages even the daylight meant the work may demand a number of the darts at that three plus two different from the process, \"you think the blurring caused by that proves nothing but the under-jets travel against . </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=3\n",
        "model = n_gram_model(n, martianOdyssey_tokenized_text)\n",
        "pre_text = 'the moon'\n",
        "print('Paragraph starting with \\\"The moon\\\" {}-gram'.format(n))\n",
        "new_paragraph(model, 100, pre_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "0DeS1K46gp8W",
        "outputId": "e7ff70bf-798a-410f-b05a-5ad3241bcfcf"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph starting with \"The moon\" 3-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lesson . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=5\n",
        "model = n_gram_model(n, martianOdyssey_tokenized_text)\n",
        "pre_text = 'the moon'\n",
        "print('Paragraph starting with \\\"The moon\\\" {}-gram'.format(n))\n",
        "new_paragraph(model, 100, pre_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "S5QJPD5Kgp-1",
        "outputId": "9df7b291-c7d5-44c4-df1e-d3a20f8702ae"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph starting with \"The moon\" 5-gram\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qpo9VKwQgqBc"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are things in the created texts that you can use to determinen wihich book is the source. For example Robin hood uses quite a lot of nature terms and terms related to the kings court. Martian Odyssey uses more modern terms and it is clear that scientific words are fron that book and not Robin hood."
      ],
      "metadata": {
        "id": "_IK5OIoihxJB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gKkb5vYEgqEA"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bpy7GEAOgqGZ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V5jL7GDtgqIv"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xFc86VjLgqLb"
      },
      "execution_count": 62,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}